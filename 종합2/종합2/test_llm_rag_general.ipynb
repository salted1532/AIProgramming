{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3194add9-2176-4556-a77e-7dbd095845cc",
   "metadata": {},
   "source": [
    "\n",
    "# RAG + LLM의 template (hf+openai)\n",
    "\n",
    "> 1) openai에서 huggingface로 전환\n",
    "> 2) 목적 : langchain기능 사용해보기, web으로 문서파일을 업로드하고, document을 읽어서, split해서, chroma에 추가하는 기능\n",
    "> 3) 확장해보기 : text, pdf등 문서읽기로 확장\n",
    "> 4) 선행작업 : HF_TOKEN, OPENAI_API_KEY 등록후 획득\n",
    "\n",
    "> 아래에는 huggingface와 openai방식으로 2개의 예제 있음.\n",
    "\n",
    "> 참고: 학원에서 제공해주는 교육 프로그램의 내용\n",
    "> https://ragmasterpath.oopy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b1e35-11ff-4f60-9829-5b9d25044df1",
   "metadata": {},
   "source": [
    "![qa image](images/chromadb.PNG)\n",
    "![qa image](images/llm.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc6f5bdf-1c9d-45e6-b6e5-dc8b97e26ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/mhkwon/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/mhkwon/.env\")\n",
    "\n",
    "import os\n",
    "\n",
    "#HF_TOKEN = \"get your token in http://hf.co/settings/tokens\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "print(HF_TOKEN)\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#hf_token = login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "# 에러가 나면, linux에서 다음 명령어를 실행\n",
    "# git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b995758-a7da-4de5-9826-377302f81984",
   "metadata": {},
   "source": [
    "\n",
    "# 0) huggingface용 llm template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "89bf1afd-81bd-46bf-8944-c52ee0ea9e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) 표준 템플릿\n",
    "# \"test_llm_general.ipynb\"에서 복사\n",
    "\n",
    "#############################################################\n",
    "# 0) 선언 부분\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# cpu/gpu를 선택 또는 지정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"cpu\"\n",
    "print('Using device:', device)\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B' # 에러 발생\n",
    "#model_id = 'LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct'\n",
    "#model_id = 'naver-clova-ix/donut-base-finetuned-docvqa' # 에러 발생\n",
    "#model_id = 'naver-clova-ix/donut-base-finetuned-cord-v2' # 에러 발생\n",
    "#model_id = 'upstage/SOLAR-10.7B-v1.0'\n",
    "#model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "#model_id = 'migtissera/Trinity-2-Codestral-22B-v0.2'\n",
    "#model_id = 'BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B'\n",
    "\n",
    "\n",
    "def my_aiquery(model_name, messages):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        #trust_remote_code=True,  # exaone only\n",
    "    )\n",
    "\n",
    "    # 결과값을 보여주는 template\n",
    "    if tokenizer.chat_template is None:\n",
    "        tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n",
    "\n",
    "    #############################################################\n",
    "    # 1) prompt과정\n",
    "    #instruction = \"철수가 20개의 연필을 가지고 있었는데 영희가 절반을 가져가고, 민수가 남은 5개를 가져갔으면 철수에게 남은 연필의 갯수는 몇개인가요?\"\n",
    "    #messages = [\n",
    "    #    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    #    ]\n",
    "    \n",
    "    #############################################################\n",
    "    # 2) tokenizer과정\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # 'unsloth/Llama-3.2-1B-Instruct 사용시에는 다음을 막아야 함.\n",
    "    if model_name == 'meta-llama/Llama-3.2-1B':\n",
    "        model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "        model.generation_config.pad_token_id = tokenizer.pad_token_id   # 설정하지 않으면, 무한 CPU 실행\n",
    "    \n",
    "    #if tokenizer.pad_token is None:\n",
    "    #    tokenizer.pad_token = tokenizer.eos_token\n",
    "    #if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    # 에러해결\n",
    "    # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. \n",
    "    # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "    \n",
    "    #terminators = [\n",
    "    #    tokenizer.eos_token_id,\n",
    "    #    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    #]\n",
    "    \n",
    "    #############################################################\n",
    "    # 3) LLM과정\n",
    "    \n",
    "    # 실행시간을 측정하는 모듈\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=200,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        #eos_token_id=terminators, \n",
    "        pad_token_id = tokenizer.eos_token_id,  # llama 3.2, bllossom\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print('elapsed time =', end_time - start_time)\n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    # 4) decoder과정\n",
    "    #answer = tokenizer.decode(outputs[0])\n",
    "    #print(answer)\n",
    "    \n",
    "    # 특수 토근을 제거하고, 출력\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    #print(response)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff42d757-7950-44cd-9b7b-812677c4092f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_autoset_attn_implementation',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_beam_search',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_check_and_enable_flash_attn_2',\n",
       " '_check_and_enable_sdpa',\n",
       " '_compiled_call_impl',\n",
       " '_constrained_beam_search',\n",
       " '_contrastive_search',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_dola_decoding',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_cache',\n",
       " '_get_candidate_generator',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_group_beam_search',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_quantized_training_enabled',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_cache_class',\n",
       " '_supports_default_dynamic_cache',\n",
       " '_supports_flash_attn_2',\n",
       " '_supports_quantized_cache',\n",
       " '_supports_sdpa',\n",
       " '_supports_static_cache',\n",
       " '_temporary_reorder_cache',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_assistant',\n",
       " '_validate_generated_length',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'active_adapter',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'contrastive_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_decoder',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'heal_tokens',\n",
       " 'hf_device_map',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'lm_head',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'main_input_name',\n",
       " 'model',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_decoder',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_output_embeddings',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'vocab_size',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22219c9-ea61-4900-9550-eb09f7550fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41913ecd-41fc-407c-83ae-688aa76c74c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a280ccee-f11b-4ef0-aa20-f652436199bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(128256, 2048)\n",
      "Linear(in_features=2048, out_features=128256, bias=False)\n",
      "meta-llama/Llama-3.2-1B\n",
      "input_ids\n",
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "print(model.get_input_embeddings())\n",
    "print(model.get_output_embeddings())\n",
    "print(model.name_or_path)\n",
    "print(model.main_input_name)\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bf1460-27fc-4da0-ba60-751194747a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 사용자질의를 이미 만들어진 chroma db에서 정보를 가져와서, chagpt에 물어보기 방식으로 만든 챗봇\n",
    "# flask4.py에서 url로 가져와서 만들어진 chroma db를 활용한다.\n",
    "\n",
    "\n",
    "# cli에서 실행\n",
    "# python flask5.py \n",
    "\n",
    "\n",
    "# 기본 개념 : https://cookbook.chromadb.dev/core/concepts/\n",
    "# gihub 소스 : https://github.com/chroma-core/chroma\n",
    "\n",
    "\n",
    "sqlite> .schema collections\n",
    "CREATE TABLE IF NOT EXISTS \"collections\" (\n",
    "    id TEXT PRIMARY KEY, -- unique globally\n",
    "    name TEXT NOT NULL, -- unique per database\n",
    "    dimension INTEGER,\n",
    "    database_id TEXT NOT NULL REFERENCES databases(id) ON DELETE CASCADE,\n",
    "    UNIQUE (name, database_id)\n",
    ");\n",
    "sqlite> \n",
    "sqlite> select * from collections;\n",
    "76e260af-b248-414b-a868-781afb6e59af|langchain|1536|00000000-0000-0000-0000-000000000000\n",
    "\n",
    "\n",
    "# https://docs.trychroma.com/guides\n",
    "\n",
    "# langchain\n",
    "# https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/\n",
    "\n",
    "# 전체 기능들:\n",
    "# https://python.langchain.com/v0.2/docs/how_to/#vector-stores\n",
    "\n",
    "# 이런 에레가 발생하면, embedding vector가 서로 다르다는 것이다.\n",
    "InvalidDimensionException: Embedding dimension 768 does not match collection dimensionality 1536\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b246996-8b4d-42ac-9e17-f34a94146dd3",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Huggingface용 chatbot-rag template\n",
    "\n",
    "[관련 vectorestore 만들고 오기](test_indexing_loader_splitter_chroma-hf.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3dcd7017-ab0c-4b59-8ce0-e67d67ebe02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  대한민국의 수도는?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapsed time = 2.169771194458008\n",
      "def calculate_sediment_volume(sediment_type, sediment_density, sediment_thickness, sediment_area):\n",
      "    \"\"\"\n",
      "    Calculate the volume of sediment in a given area.\n",
      "\n",
      "    Args:\n",
      "        sediment_type (str): The type of sediment (e.g. \"sand\", \"silt\", \"clay\").\n",
      "        sediment_density (float): The density of the sediment in kg/m^3.\n",
      "        sediment_thickness (float): The thickness of the sediment in meters.\n",
      "        sediment_area (float): The area of the sediment in square meters.\n",
      "\n",
      "    Returns:\n",
      "        float: The volume of the sediment in cubic meters.\n",
      "    \"\"\"\n",
      "    # Calculate the volume of the sediment in cubic meters\n",
      "    volume = sediment_area * sediment_density * sediment_thickness\n",
      "\n",
      "    # Check if the sediment is of a specific type\n",
      "    if sediment_type == \"sand\":\n",
      "        # Calculate the volume of the sand in cubic meters\n",
      "        sand_volume = volume * 0.5\n",
      "        # Check if the sand volume is\n",
      "\n",
      "\n",
      "Source documents:\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "#1) Huggingface용 chatbot-rag template\n",
    "\n",
    "# \"test_indexing_loader_splitter_chroma-hf.ipynb\"를 통해서 만들어진\n",
    "# vectorstore \"chroma_store_hf\"를 사용한다.\n",
    "\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model_name = 'meta-llama/Llama-3.2-1B'\n",
    "\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "   \n",
    "vectorstore = Chroma(persist_directory='chroma_store_hf', embedding_function = hf_embeddings)\n",
    "\n",
    "def build_prompt(query, context):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"I am going to ask you a question, which I would like you to answer\"\n",
    "            \"based only on the provided context, and not any other information.\"\n",
    "            \"If there is not enough information in the context to answer the question,\"\n",
    "            'say \"I am not sure\", then try to make a guess.'\n",
    "            \"Break your answer up into nicely readable paragraphs.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            f\"The question is {query}. Here is all the context you have:\"\n",
    "            #f'{(\" \").join(context)}',\n",
    "            f'{context}',\n",
    "        ),\n",
    "    ])\n",
    "    #print(prompt)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_chatHF_response(query, context, model_name):\n",
    "    response = my_aiquery(\n",
    "        model_name=model_name,\n",
    "        messages=build_prompt(query, context),)\n",
    "    #print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Get the user's query\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "    print(f\"\\nThinking using {model_name}...\\n\")\n",
    "    \n",
    "    # Query the collection to get the 5 most relevant results\n",
    "    results = vectorstore.search(\n",
    "        query=query, \n",
    "        search_type=\"similarity\", # \"mmr\", \"similarity_score_threshold\"\n",
    "        k=5,\n",
    "        #include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sources = \"\\n\".join(\n",
    "        [   f\"{result.metadata['source']}\"\n",
    "            for result in results \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the response from GPT\n",
    "    res = ''\n",
    "    if len(results) > 0:\n",
    "        res = results[0].page_content\n",
    "        \n",
    "    response = get_chatHF_response(query, res, model_name)  # type: ignore\n",
    "\n",
    "    # Output, with sources\n",
    "    print(response)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Source documents:\\n{sources}\")\n",
    "    print(\"----\"*20)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fabe10-a396-40f2-84f6-699a9df72f50",
   "metadata": {},
   "source": [
    "\n",
    "# 2) OpenAI용 template\n",
    "\n",
    "\n",
    "[관련 vectorestore 만들고 오기](test_indexing_loader_splitter_chroma.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9924df74-d291-48a0-a5a0-3df1d104767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  한국의 수도는?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using gpt-3.5-turbo...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Based on the limited context provided, the question \"한국의 수도는?\" translates to \"What is the capital of South Korea?\" The answer to this question is \"서울 (Seoul).\" Seoul is the capital city of South Korea and the largest metropolis in the country. It serves as the political, cultural, and economic center of South Korea, making it the capital city. Therefore, based on the context given, the answer to the question is 서울 (Seoul).\n",
      "\n",
      "\n",
      "Source documents:\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "https://www.etnews.com/20240823000244\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 버전\n",
    "\n",
    "# \"test_indexing_loader_splitter_chroma.ipynb\"를 통해서 만들어진 vectorstore를 사용한다.\n",
    "\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from openai.types.chat import ChatCompletionMessageParam\n",
    "import openai\n",
    "\n",
    "model_id = \"gpt-3.5-turbo\"\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore_openai = Chroma(persist_directory='chroma_store', embedding_function = embeddings)\n",
    "\n",
    "def build_prompt_openai(query, context):\n",
    "    system: ChatCompletionMessageParam = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"I am going to ask you a question, which I would like you to answer\"\n",
    "        \"based only on the provided context, and not any other information.\"\n",
    "        \"If there is not enough information in the context to answer the question,\"\n",
    "        'say \"I am not sure\", then try to make a guess.'\n",
    "        \"Break your answer up into nicely readable paragraphs.\",\n",
    "    }\n",
    "    user: ChatCompletionMessageParam = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"The question is {query}. Here is all the context you have: \"\n",
    "        \"{context}\"\n",
    "        #f'{(\" \").join(context)}',\n",
    "    }\n",
    "\n",
    "    return [system, user]\n",
    "\n",
    "def get_chatGPT_response(query, context, model_id):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=build_prompt_openai(query, context),\n",
    "    )\n",
    "    #print(response)\n",
    "\n",
    "    return response.choices[0].message.content  # type: ignore\n",
    "######\n",
    "\n",
    "    \n",
    "vectorstore = Chroma(persist_directory='chroma_store', embedding_function = embeddings)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get the user's query\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "    print(f\"\\nThinking using {model_id}...\\n\")\n",
    "    \n",
    "    # Query the collection to get the 5 most relevant results\n",
    "    results = vectorstore.search(\n",
    "        query=query, \n",
    "        search_type=\"similarity\", # \"mmr\", \"similarity_score_threshold\"\n",
    "        #n_results=5, # 아래 k로 대체\n",
    "        k=5, \n",
    "        #include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    sources = \"\\n\".join(\n",
    "        [   f\"{result.metadata['source']}\"\n",
    "            for result in results \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the response from GPT\n",
    "    res = ''\n",
    "    if len(results) > 0:\n",
    "        res = results[0].page_content\n",
    "        \n",
    "    response = get_chatGPT_response(query, res, model_id)  # type: ignore\n",
    "    \n",
    "    # Output, with sources\n",
    "    print(response)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Source documents:\\n{sources}\")\n",
    "    print(\"----\"*20)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0aae749-4ad5-4a9a-8cfa-f236b53a2284",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, Document found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, Document found"
     ]
    }
   ],
   "source": [
    "f'{(\" \").join(results)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01145f95-e205-4f4c-b599-3b2aede0b0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.etnews.com/20240823000244\\nhttps://www.etnews.com/20240823000244\\nhttps://www.etnews.com/20240823000244\\nhttps://www.etnews.com/20240823000244\\nhttps://www.etnews.com/20240823000244'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources = \"\\n\".join(\n",
    "    [   f\"{result.metadata['source']}\"\n",
    "        for result in results \n",
    "    ]\n",
    ")\n",
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03de1b79-dbbe-4986-86d3-500ce1b79e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': '미래를 보는 창 - 전자신문', 'language': 'ko', 'source': 'https://www.etnews.com/20240823000244', 'title': \"카카오브레인, AI 헬스케어 전담 '씨엑스알랩'으로 분할한다 - 전자신문\"}, page_content='차 부수고 시민 위협한 20대 음주 운전자뉴스속보SWIT경제전자모빌리티플랫폼/유통과학\\xa0정치오피니언국제전국스포츠특집연재라이프연예포토공연전시생활문화비주얼IT이슈플러스Hot 영상뷰포인트인포그래픽부가서비스ConferenceallshowTV시사용어PDF서비스서비스안내신문구독신청콘텐츠구매초판서비스회원서비스내 스크랩이용안내지면광고안내행사문의디지털광고안내이용약관개인정보취급방침고충처리회사소개전자신문전자신문인터넷연혁CI소개회사위치 지면광고안내행사문의디지털광고안내이용약관개인정보취급방침고충처리사이트맵전자신문회사소개주소 : 서울시 서초구 양재대로2길 22-16 호반파크1관대표번호 : 02-2168-9200등록번호 : 서울 아04494등록일자 : 2017년 04월 27일사업자명 : 전자신문인터넷사업자번호 : 107-81-80959발행·편집인: 심규호청소년보호책임자: 김인기Copyright © Electronic Times Internet. All Rights Reserved.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915f8a31-2e1a-4d9a-b922-e828681d850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': '미래를 보는 창 - 전자신문',\n",
       " 'language': 'ko',\n",
       " 'source': 'https://www.etnews.com/20240823000244',\n",
       " 'title': \"카카오브레인, AI 헬스케어 전담 '씨엑스알랩'으로 분할한다 - 전자신문\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fca5aa3-e505-4c9b-9472-a4c604f1b748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"카카오브레인, AI 헬스케어 전담 '씨엑스알랩'으로 분할한다 - 전자신문\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].metadata['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e75de5-7127-42b0-9731-41329bb04e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymychabot",
   "language": "python",
   "name": "mychabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
