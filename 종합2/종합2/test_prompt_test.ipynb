{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462cef1e-cc7a-4273-8329-735af5240540",
   "metadata": {},
   "source": [
    "\n",
    "# prompt í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# ì—¬ëŸ¬ëª¨ë¸ì—, ë‹¤ì–‘í•œ ì§ˆì˜ë¡œ ì‘ë‹µì˜ ì •í™•ì„±ì„ í…ŒìŠ¤íŠ¸ í•´ë³´ê¸°\n",
    "\n",
    ">1) ë‹¨ìˆœ ì§ˆì˜í•˜ê¸°\n",
    ">2) ì´ì „ ì§ˆì˜.ì‘ë‹µì •ë³´ë¥¼ ì¶”ê°€í•˜ê¸°\n",
    ">3) ë‹¨ìˆœ ì •ë³´ë¥¼ ì¶”ê°€í•´ì„œ, ì§ˆì˜í•˜ê¸°\n",
    ">4) ë³µí•© ì •ë³´ë¥¼ ì¶”ê°€í•´ì„œ, ì§ˆì˜í•˜ê¸°\n",
    ">5) ì˜ì–´ ë°”ê¾¸ì–´ì„œ ì§ˆì˜í•´ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1c829e-c2d2-43e7-91d6-7bc3f8cff78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/mhkwon/.env\")\n",
    "\n",
    "import os\n",
    "\n",
    "#HF_TOKEN = \"get your token in http://hf.co/settings/tokens\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "print(HF_TOKEN)\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#hf_token = login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "# ì—ëŸ¬ê°€ ë‚˜ë©´, linuxì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰\n",
    "# git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8df4a1-c2e9-45cf-8723-20ce7591d04a",
   "metadata": {},
   "source": [
    "\n",
    "# 1) ë‹¤ì–‘í•œ LLM MODELì„ local, api ë°©ì‹ìœ¼ë¡œ ì„¤ì •í•´ì„œ ì‚¬ìš©í• ìˆ˜ ìˆëŠ” ì½”ë“œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b4bab3-5c62-4e35-b204-720e382bc440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒê³¼ ê°™ì´ í•´ë„ ëœë‹¤.\n",
    "## \"my_common/my_llm.py\"\n",
    "#from my_common.my_llm import *\n",
    "\n",
    "###############################\n",
    "# \"test_rag_all_llm_general.ipynb\" ì—ì„œ ë³µì‚¬\n",
    "\n",
    "# LLM template \n",
    "\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "#from langchain_community.llms.ollama import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "#from langchain_community.chat_models import ChatOllama\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# ì‹¤í–‰ì‹œê°„ì„ ì¸¡ì •í•˜ëŠ” ëª¨ë“ˆ\n",
    "import time\n",
    "\n",
    "# RAGí˜•ì‹ì„ ì²˜ë¦¬ìœ„í•œ, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Basing only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the following question: {question}\n",
    "Avoid to start the answer saying that you are basing on the provided context and go straight with the response.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE_NAIVE = \"\"\"\n",
    "Answer the following question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE_EMPTY = \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "class LLM(ABC):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def generate_response(self, context: str, question: str, is_template=True) -> str:\n",
    "        if context is not None:\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "        elif is_template:\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_NAIVE)\n",
    "            prompt = prompt_template.format(question=question)\n",
    "        else:\n",
    "            #prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_EMPTY)\n",
    "            #prompt = prompt_template.format(question=question)\n",
    "            prompt = question\n",
    "        #\n",
    "        #print(prompt)\n",
    "        \n",
    "        response_text, elapsed_time = self.invoke(prompt)\n",
    "        return response_text, elapsed_time\n",
    "\n",
    "class OllamaModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model = OllamaLLM(model=model_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        start_time = time.time()\n",
    "        response = self.model.invoke(prompt)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return response, elapsed_time\n",
    "\n",
    "    def __del__(self):\n",
    "        self.model = OllamaLLM(model=self.model_name, keep_alive=0)\n",
    "        #print(\"deleted..\")\n",
    "        \n",
    "#To unload the model and free up memory use:\n",
    "#curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n",
    "        \n",
    "class GPTModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        return response.choices[0].message.content.strip(), elapsed_time\n",
    "    \n",
    "class AnthropicModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract the plain text from the response content\n",
    "        text_blocks = response.content\n",
    "        plain_text = \"\\n\".join(block.text for block in text_blocks if block.type == 'text')\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return plain_text, elapsed_time\n",
    "\n",
    "\n",
    "# 1) í‘œì¤€ í…œí”Œë¦¿\n",
    "# \"test_llm_general.ipynb\"ê³¼ \"test_llm_rag_general.ipynb\"ì—ì„œ ë³µì‚¬ ë° ìˆ˜ì •í•œ ì½”ë“œ\n",
    "\n",
    "#############################################################\n",
    "# 0) ì„ ì–¸ ë¶€ë¶„\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class LocalModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # ê²°ê³¼ê°’ì„ ë³´ì—¬ì£¼ëŠ” template\n",
    "        if self.tokenizer.chat_template is None:\n",
    "            self.tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n",
    "        #if tokenizer.pad_token is None:\n",
    "        #    tokenizer.pad_token = tokenizer.eos_token\n",
    "        #if tokenizer.pad_token_id is None:\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        # ì—ëŸ¬í•´ê²°\n",
    "        # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. \n",
    "        # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "        \n",
    "        self.client = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            #trust_remote_code=True,  # exaone only\n",
    "        )\n",
    "        # 'unsloth/Llama-3.2-1B-Instruct ì‚¬ìš©ì‹œì—ëŠ” ë‹¤ìŒì„ ë§‰ì•„ì•¼ í•¨.\n",
    "        if model_name == 'meta-llama/Llama-3.2-1B':\n",
    "            self.client.generation_config.pad_token_id = self.client.generation_config.eos_token_id\n",
    "            self.client.generation_config.pad_token_id = self.tokenizer.pad_token_id   # ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´, ë¬´í•œ CPU ì‹¤í–‰\n",
    "        \n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        #############################################################\n",
    "        # 1) promptê³¼ì •\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        #############################################################\n",
    "        # 2) tokenizerê³¼ì •\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.client.device)\n",
    "               \n",
    "        #############################################################\n",
    "        # 3) LLMê³¼ì •\n",
    "        start_time = time.time()\n",
    "        outputs = self.client.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,  # llama 3.2, bllossom\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #############################################################\n",
    "        # 4) decoderê³¼ì •\n",
    "        answer = self.tokenizer.decode(outputs[0])\n",
    "    \n",
    "        # íŠ¹ìˆ˜ í† ê·¼ì„ ì œê±°í•˜ê³ , ì¶œë ¥\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "        return response, elapsed_time\n",
    "\n",
    "# LLM facotry\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def create_llm(model_type: str, model_name: str, api_key: str = None) -> LLM:\n",
    "        if model_type == 'local':\n",
    "            return LocalModel(model_name)\n",
    "        elif model_type == 'ollama':\n",
    "            return OllamaModel(model_name)\n",
    "        elif model_type == 'gpt':\n",
    "            return GPTModel(model_name, api_key)\n",
    "        elif model_type == 'claude':\n",
    "            return AnthropicModel(model_name, api_key)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da4aaf8-b078-4e45-bd7e-aebb6466398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36240/1875692954.py:70: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.model = Ollama(model=model_name)\n"
     ]
    }
   ],
   "source": [
    "# ìœ„ì˜ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸í•˜ê¸°\n",
    "\n",
    "LLM_MODEL_TYPE = \"ollama\"  # 'local', 'ollama', 'gpt' or 'claude'\n",
    "LLM_MODEL_NAME = 'llama3.2' # \"llama3.2\"\n",
    "                     # 'local' = meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "                     # 'ollama' = 'llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b', \n",
    "                     # 'gpt' = 'gpt-3.5-turbo', 'GPT-4o' \n",
    "                     # 'claude' = 'claude'\n",
    "api_key = HF_TOKEN\n",
    "#\n",
    "\n",
    "model = LLMFactory.create_llm(model_type=LLM_MODEL_TYPE, model_name=LLM_MODEL_NAME, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445c7e1-91c3-4c96-9df2-a32739565a82",
   "metadata": {},
   "source": [
    "\n",
    "# 2.1) í™”ë©´ì—ì„œ, ì§ˆì˜ ì‘ë‹µì„ ë°˜ë³µ ë°©ë²•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512681b3-569c-49fb-a1b0-efe75050cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "# 2.1) ì§ˆì˜ ì‘ë‹µ ë°˜ë³µ ë°©ë²•\n",
    "\n",
    "while True:\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "    \n",
    "    enhanced_context_text = ['make one answer']\n",
    "    response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query)\n",
    "\n",
    "    # Output, with sources\n",
    "    print(\"----\"*20)\n",
    "    print(f\"elapse time = {elapsed_time}\\n, response = [{response}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b720bc-909e-4189-977a-296e48230ce3",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2) 1ê°œ ì§ˆì˜ì— ì‘ë‹µí•˜ëŠ” ë°©ë²•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb52d93-ae13-42dc-b2ea-34331063fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using llama3.2...\n",
      "\n",
      "ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\n",
      "elapse time = 0.39\n",
      ", response = [ì»¤í”¼ì˜ ê°€ê²©ì€ ì—¬ëŸ¬ ìš”ì¸ì— ë”°ë¼ ë‹¤ë¥´ë©°, ì§€ì—­, quÃ¡n ì´ë¦„, ì»¤í”¼ ì¢…ë¥˜ ë“±ì´å½±éŸ¿í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œå†° coffeeëŠ” 2,000 ~ 5,000 ì›ä¹‹é—´ì´ì „ì´ë¼ê³  ìƒê°ë©ë‹ˆë‹¤.]\n"
     ]
    }
   ],
   "source": [
    "# 2.2) 1ê°œì”© ì§ˆì˜ ì‘ë‹µí•˜ëŠ” ë°©ë²•\n",
    "\n",
    "query = \"ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\"\n",
    "\n",
    "print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "\n",
    "#enhanced_context_text = ['make one answer']\n",
    "enhanced_context_text = None\n",
    "response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query, is_template=False)\n",
    "\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response}]\")\n",
    "#print(\"----\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737a1b6-887a-48d4-9a53-fec5fa07866b",
   "metadata": {},
   "source": [
    "\n",
    "# 3) ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¥¼ í•˜ê¸° ìœ„í•œ ì‚¬ì „ ì„¤ì •ê°’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a9298e-8776-4cd0-9e43-0e420cf261b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types_table = {'local', 'ollama', }# 'gpt', } # 'claude'}\n",
    "model_names_table = {\n",
    "                     'local':  ['meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-2.8B-v1.0'],\n",
    "                     'ollama': ['llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b',],\n",
    "                     'gpt' :   ['gpt-3.5-turbo'], # 'GPT-4o' ],\n",
    "                     'claude': ['claude'],\n",
    "                    }\n",
    "\n",
    "api_key_table = {'local': os.getenv(\"HF_TOKEN\"),\n",
    "                 'ollama': os.getenv(\"HF_TOKEN\"),\n",
    "                 'gpt': os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 'claude': os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                }\n",
    "\n",
    "\n",
    "###################################################################\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "def calc_coffee(query,enhanced_context_text,is_template):\n",
    "    for model_type in model_types_table:\n",
    "        api_key = api_key_table[model_type]\n",
    "        for model_name in model_names_table[model_type]:\n",
    "            print(f\"\\nThinking using {model_type}/{model_name}...\\n\")\n",
    "            model = LLMFactory.create_llm(model_type=model_type, model_name=model_name, api_key=api_key)\n",
    "            response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query, is_template=is_template)\n",
    "    \n",
    "            # \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            process = psutil.Process()\n",
    "            usage = process.memory_info().rss \n",
    "            \n",
    "            #print(f'GPU allocated = {torch.cuda.memory_allocated()/1024**2}, \\\n",
    "            #        GPU reserved = {torch.cuda.memory_reserved()/1024**2}, \\\n",
    "            #        CPU Mem used = {usage/1024**2}')\n",
    "    \n",
    "            print(f\"elapse time = {elapsed_time:.2f}\\nresponse = [{response}]\")\n",
    "            print(\"----\"*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66911e9b-ba5d-4068-abe6-7e566b1c6513",
   "metadata": {},
   "source": [
    "\n",
    "# 3.1) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì •  (100% ì§ˆì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c511f020-97e6-4388-9956-f068495636b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.35\n",
      "response = [ice coffeeëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë” ë¹„ì‹¸ê¸° ë•Œë¬¸ì— ì €ëŠ” 10,000ì›ì—ì„œ 15,000ì›ä¹‹é—´ì˜ ê°€ê²©ì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 2.49\n",
      "response = [ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ, ì €ëŠ” ì‹¤ì œ ì„¸ê³„ì˜ ê°€ê²© ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ì´ìŠ¤ ì»¤í”¼ ê°€ê²©ì€ ë§¤ì¥, ìœ„ì¹˜, í¬ê¸°, ì²¨ê°€ì¬ ë“± ì—¬ëŸ¬ ìš”ì¸ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤.\n",
      "\n",
      "ê°€ì¥ ì •í™•í•œ ì •ë³´ëŠ” í•´ë‹¹ ë§¤ì¥ìœ¼ë¡œ ì§ì ‘ ë¬¸ì˜í•˜ì‹œê±°ë‚˜ ì˜¨ë¼ì¸ ë©”ë‰´ì—ì„œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 3.02\n",
      "response = [ğŸ˜Š\n",
      "\n",
      "The price of an iced coffee at a cafe or coffee shop can vary depending on the location, size, and type of coffee. Here are some rough estimates:\n",
      "\n",
      "* Small (8-10 oz): â‚©2,000 to â‚©3,500 (~$1.70 to $2.90 USD)\n",
      "* Medium (12-14 oz): â‚©3,000 to â‚©5,000 (~$2.50 to $4.20 USD)\n",
      "* Large (16-18 oz): â‚©4,000 to â‚©6,500 (~$3.30 to $5.40 USD)\n",
      "\n",
      "Keep in mind that prices can be higher or lower depending on the cafe, its location, and any additional features like whipped cream, flavorings, or syrups.\n",
      "\n",
      "Would you like me to suggest some popular coffee shops in Seoul where you can get a delicious iced coffee? ğŸ¤”]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 2.30\n",
      "response = [í•œ ëŒ€ï¿½ennen ì•„ì´ìŠ¤ ì»¤í”¼ì˜ ì–‘ì€ êµ­ê°€ì™€ ì˜ì—…ì¥ì— ë”°ë¼ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•œ ëŒ€ï¿½innen ì•„ì´ìŠ¤ ì»¤í”¼ëŠ” 300-500mlì˜ í¬ê¸°ë¥¼ ì§€ë‹™ë‹ˆë‹¤. ë¬¼ê³¼ ì¹´í˜ì¸ ì¶”ì¶œì œ ë¹„ìœ¨ì— ë”°ë¼ì„œëŠ” ë³€ë™ì´ ìˆìœ¼ë¯€ë¡œ, ì •í™•í•œ ì–‘ì€ ì§ì ‘ ì˜ˆì•½í•˜ì‹  ì˜ì—…ì¥ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.26\n",
      "response = [ 20.00\n",
      "- 1,100.00ì›\n",
      "- 30.00ì›\n",
      "- 50.00ì›\n",
      "- 100.00ì›\n",
      "- 150.00ì›\n",
      "- 200.00ì›\n",
      "- 250.00ì›\n",
      "- 300.00ì›\n",
      "- 400.00ì›\n",
      "- 500.00ì›\n",
      "- 600.00ì›\n",
      "- 700.00ì›\n",
      "- 800.00ì›\n",
      "- 900.00ì›\n",
      "- 1000.00ì›\n",
      "- 1100.00ì›\n",
      "- 1200.00ì›\n",
      "- 1300.00ì›\n",
      "- 1400.00ì›\n",
      "- 1500.00ì›\n",
      "- 1600.00ì›\n",
      "- 1700.00ì›\n",
      "- 1800.00ì›\n",
      "- 1900.00ì›\n",
      "- 2000.00ì›\n",
      "- 2100.00]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039e0ea310014220aeec2c3eae527637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.22\n",
      "response = [ì •í™•íˆ 1ì”ì— ì–¼ë§ˆê°€ ë˜ë‚˜ìš”?]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.1) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì •  (100% ì§ˆì˜ ë‚´ìš©ë§Œìœ¼ë¡œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n",
    "\n",
    "\n",
    "\n",
    "calc_coffee(query = \"ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\",\n",
    "            enhanced_context_text = None,\n",
    "            is_template=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90a311-45cf-45a9-9885-fa767614d5b5",
   "metadata": {},
   "source": [
    "\n",
    "# 3.2) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì •  (templateë°©ì‹ìœ¼ë¡œ ì§ˆì˜)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a492b04-d0c8-4eb3-a889-12183ebdf64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.21\n",
      "response = [\"ì˜¤ëŠ˜ì€ 8,000ì›ì…ë‹ˆë‹¤. ì°¨ê°€ìš´ ì•„ì´ìŠ¤ ì»¤í”¼ë¥¼ ë“œì‹œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\"]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 3.18\n",
      "response = [I need more information to answer your question. \"ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”\" means \"one iced coffee\" in Korean, but I need to know where you want to buy it!\n",
      "\n",
      "Please tell me:\n",
      "\n",
      "* **The location:** Is it a specific cafe, a general area, or a type of store (e.g., convenience store)?\n",
      "* **Size:**  Is it a small, medium, or large iced coffee?\n",
      "\n",
      "\n",
      "With more details, I can help you find the price! ğŸ˜Š  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.73\n",
      "response = [A popular Korean phrase!\n",
      "\n",
      "The answer is: ì¼ë°˜ì ìœ¼ë¡œ 2,000~3,000ì› (around $1.80-$2.70 USD) per cup.\n",
      "\n",
      "Of course, prices may vary depending on the location, cafe, or quality of the coffee. But generally speaking, a standard-sized iced coffee in Korea would cost around 2,000-3,000 won.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.37\n",
      "response = [ì¼ë°˜ì ìœ¼ë¡œ ì•„ì´ìŠ¤ ì»¤í”¼ëŠ” í•œ ì”ì— 250ml ~ 350mlì˜ ì–‘ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ 1 wonìœ¼ë¡œ 1mlì˜ ê°€ê²©ì„ ê°€ì •í•˜ë©´, ì¼ë°˜ì ì¸ ì•„ì´ìŠ¤ ì»¤í”¼ í•œ ì”ì˜ ê°€ê²©ì€ 250ml * 1won/ml ~ 350ml * 1won/ml = 250won ~ 350wonì…ë‹ˆë‹¤.\n",
      "\n",
      "í•˜ì§€ë§Œ ì‹¤ì œ ê°€ê²©ì€ ì‹œì¥, ë¸Œëœë“œ, ì§€ì—­ ë“±ì˜ ìš”ì¸ì— ë”°ë¼ ì°¨ì´ê°€ ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.36\n",
      "response = [-;\n",
      "\n",
      "```julia\n",
      "julia> push!(x, i+1)\n",
      "julia> push!(x, i+2)\n",
      "julia> push!(x, i+3)\n",
      "```\n",
      "* xëŠ” `i` ê°’ì— 1ì”© ì¦ê°€í•œë‹¤.\n",
      "\n",
      "> `x`ëŠ” `1`ì´ë‹ˆ iê°€ 2ë¡œ ì¦ê°€í•˜ë©´ [1,2,3,4]ê°€ ëœë‹¤.\n",
      "\n",
      "## push\n",
      "\n",
      "* push!() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ìƒˆ ê°’ì„ `x`ì— ë”í•´ì¤€ë‹¤.\n",
      "\n",
      "## pop\n",
      "\n",
      "* pop!() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´, `x`ì—ì„œ ê°€ì¥ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°’ì„ ì œê±°í•˜ê³  ë°˜í™˜í•œë‹¤.\n",
      "\n",
      "> xê°€ [1,2,3,4,5]ë¼ë©´ ê°€ì¥ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°’ì€ [1,2,3,4]ê°€ ë˜ê³  xëŠ” [1,2,3,5]ê°€ ëœë‹¤.\n",
      "\n",
      "## push!()\n",
      "\n",
      "* push!() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´, ìƒˆ]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5582aa93e94497fb53f5a3e110cff8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 13.34\n",
      "response = [Human: \n",
      "Answer the following question: ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\n",
      "Answer: 2.5 ìº”\n",
      "<|im_start|>theorem\n",
      "You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.2) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì •  (templateë°©ì‹ìœ¼ë¡œ ì§ˆì˜)\n",
    "\n",
    "\n",
    "calc_coffee(query = \"ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\",\n",
    "            enhanced_context_text = None,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d5ea-b493-4bfb-a223-786703cf5c01",
   "metadata": {},
   "source": [
    "\n",
    "# 3.3) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì • (template + contextë¥¼ ë³´ì¶©í•´ì„œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a5114c7-d7d2-411b-8e8e-1f5e7711e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.82\n",
      "response = [ì»¤í”¼ í•œ ì”ì— 1000ì›ì…ë‹ˆë‹¤. ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì› ë” ì¶”ê°€ë˜ë¯€ë¡œ ì•„ì´ìŠ¤ ì»¤í”¼ì˜ ì´ ê°€ê²©ì€ 1000 + 500 = 1500ì›ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.95\n",
      "response = [1500ì›  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.20\n",
      "response = [1,000ì› plus 500ì› is... 1,500ì›!]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 0.35\n",
      "response = [1500ì› ì…ë‹ˆë‹¤. (Assuming a base price of 1000ì› for coffee, and an additional charge of 500won for ice.)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.28\n",
      "response = [#1#.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 1: 5000ì›\n",
      "\n",
      "In the 5000ì› case, if we were to assume that the following are all the options:\n",
      "\n",
      "* ì»¤í”¼ëŠ” 1ì”ì— 1000ì›\n",
      "* ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì›ì´ ì¶”ê°€ë©ë‹ˆë‹¤.\n",
      "\n",
      "We would say that 'ì»¤í”¼ëŠ” 1ì”ì— 1000ì›' is the correct answer.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 2: 5000ì›\n",
      "\n",
      "In the 5000ì› case, if we were to assume that the following are all the options:\n",
      "\n",
      "* ì»¤í”¼ëŠ” 1ì”ì— 1000ì›\n",
      "* ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì›ì´ ì¶”ê°€ë©ë‹ˆë‹¤.\n",
      "\n",
      "We would say that 'ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì›ì´ ì¶”ê°€ë©ë‹ˆë‹¤.' is the correct answer.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 3: 1ì”ì— 1000ì›\n",
      "\n",
      "In the]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769208c1ada4417b60c314a8dfe051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 14.05\n",
      "response = [\n",
      "I'm sorry for the misunderstanding. â€œì•„ì´ìŠ¤ ì»¤í”¼â€ëŠ” \"1ì”ì— 1000ì›, ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì›ì´ ì¶”ê°€ë©ë‹ˆë‹¤.â€ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.3) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì • (template + contextë¥¼ ë³´ì¶©í•´ì„œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n",
    "\n",
    "\n",
    "calc_coffee(query = \"ì•„ì´ìŠ¤ ì»¤í”¼ í•œì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\",\n",
    "            enhanced_context_text = ['ì»¤í”¼ëŠ” 1ì”ì— 1000ì›, ì»¤í”¼ì— ì•„ì´ìŠ¤ê°€ ì¶”ê°€ë˜ë©´ 500ì›ì´ ì¶”ê°€ë©ë‹ˆë‹¤.'],\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c82994-f970-4a46-b030-4a07c85a86ea",
   "metadata": {},
   "source": [
    "\n",
    "# 3.4) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì • (template + ë” ë³µì¡í•œ contextë¥¼ ë³´ì¶©í•´ì„œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb01cc39-864b-4b0d-82c8-5593687c8547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.85\n",
      "response = [ì¹´í˜ë¼í…Œ 1 ì”ì˜ ê°€ê²©ì€ 1200ì›ì…ë‹ˆë‹¤. ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì› ì¶”ê°€ë©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì˜ ê°€ê²©ì€ (1200 + 500) * 3 = 5100ì›ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 0.19\n",
      "response = [4200ì›  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 1.04\n",
      "response = [A nice question!\n",
      "\n",
      "Ice adds 500 won to each drink, so for Ice Cappuccino (not Cafe Rata), it would be:\n",
      "\n",
      "1st cup: 1300 won + 500 won = 1800 won\n",
      "2nd cup: 1300 won + 500 won = 1800 won\n",
      "3rd cup: 1300 won + 500 won = 1800 won\n",
      "\n",
      "So, the total price for Ice Cappuccino 3 cups is: 1800 won x 3 = 5400 won.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.52\n",
      "response = [4700ì›. (Assuming the base price of a latte is 1200 won, plus an additional 500 won for ice per drink.)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 0.57\n",
      "response = [Question\n",
      "1. What is the word of the day?\n",
      "1. The correct answer to this question is: ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ\n",
      "\n",
      "Answer\n",
      "1. The correct answer to this question is: ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76b490929cf4665af788b2ce7eda995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.17\n",
      "response = [You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.4) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì • (template + contextë¥¼ ë³´ì¶©í•´ì„œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n",
    "\n",
    "# ==> ë˜‘ê°™ì€ ì§ˆì˜ë¡œ ë°˜ë³µí•˜ë©´, ë‹µì´ ì™”ë‹¤ ê°”ë‹¤ í•œë‹¤.\n",
    "\n",
    "query = \"ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\"\n",
    "#enhanced_context_text = None\n",
    "enhanced_context_text = [\n",
    "    # ì‹œë„1\n",
    "    # 'ì•„ë©”ë¦¬ì¹´ë…¸ 1ì”ì— 1000ì›','ì¹´í˜ë¼ë–¼ 1ì”ì— 1200ì›','ì—ìŠ¤í”„ë ˆì†Œ 1ì”ì— 1400ì›', 'ì¹´í‘¸ì¹˜ë…¸ 1ì”ì— 1300ì›','ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì›'\n",
    "    # ì •ë‹µ ì‚°ì¶œ ëª¨ë¸ : ollama/llama3:8b...\n",
    "    \n",
    "    # ì‹œë„2\n",
    "    # ì •ë‹µ ì‚°ì¶œ ëª¨ë¸ : ollama/llama3.2, ollama/llama3:8b\n",
    "    #'ì•„ë©”ë¦¬ì¹´ë…¸ 1ì”ì˜ ê°€ê²©ì€ 1000ì›','ì¹´í˜ë¼ë–¼ 1ì”ì˜ ê°€ê²©ì€  1200ì›','ì—ìŠ¤í”„ë ˆì†Œ  1ì”ì˜ ê°€ê²©ì€ 1400ì›', \n",
    "    #'ì¹´í‘¸ì¹˜ë…¸  1ì”ì˜ ê°€ê²©ì€ 1300ì›','ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì›'\n",
    "\n",
    "    # ì‹œë„3\n",
    "    # ì •ë‹µ ì‚°ì¶œ ëª¨ë¸ : ollama/llama3:8b\n",
    "    #'ì•„ë©”ë¦¬ì¹´ë…¸ 1ì”ì˜ ê°€ê²©ì€ 1000ì›ì…ë‹ˆë‹¤.','ì¹´í˜ë¼ë–¼ 1ì”ì˜ ê°€ê²©ì€  1200ì›ì…ë‹ˆë‹¤.','ì—ìŠ¤í”„ë ˆì†Œ  1ì”ì˜ ê°€ê²©ì€ 1400ì›ì…ë‹ˆë‹¤.', \n",
    "    #'ì¹´í‘¸ì¹˜ë…¸  1ì”ì˜ ê°€ê²©ì€ 1300ì›ì…ë‹ˆë‹¤.','ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì› ì¶”ê°€ë©ë‹ˆë‹¤.'\n",
    "\n",
    "    # ì‹œë„4\n",
    "    # ì •ë‹µ ì‚°ì¶œ ëª¨ë¸ : ollama/llama3.2\n",
    "    'ì•„ë©”ë¦¬ì¹´ë…¸ 1ì”ì˜ ê°€ê²©ì€ 1000ì›.','ì¹´í˜ë¼ë–¼ 1ì”ì˜ ê°€ê²©ì€  1200ì›.','ì—ìŠ¤í”„ë ˆì†Œ  1ì”ì˜ ê°€ê²©ì€ 1400ì›.', \n",
    "    'ì¹´í‘¸ì¹˜ë…¸  1ì”ì˜ ê°€ê²©ì€ 1300ì›.','ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì› ì¶”ê°€ë©ë‹ˆë‹¤.'\n",
    "]\n",
    "\n",
    "# ì •ë‹µì€ 3 x (1200 + 500) = 5100ì›\n",
    "\n",
    "\n",
    "\n",
    "# 1ì°¨ ì‹œë„\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a05cf5a7-44de-4f30-b56b-f576b09e5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.29\n",
      "response = [ì¹´í˜ë¼í…Œ 1ì”ì— 1200ì›ì…ë‹ˆë‹¤. therefore, ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì˜ ê°€ê²©ì€ 1200ì› x 3 = 3600ì›ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.95\n",
      "response = [4800ì›  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.70\n",
      "response = [ì´ì‚¬ ì¹´í˜ë¼ë–¼ 1ì”ì— 1200ì›ì´ê³ , ì•„ì´ìŠ¤ë¥¼ ì¶”ê°€í•˜ë©´ 500ì›ì´ ë” ë“¤ì–´ê°‘ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ì´ìŠ¤ ì¹´í˜ë¼ë–¼ 1ì”ì€ 1700ì›ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ì•„ì´ìŠ¤ ì¹´í˜ë¼ë–¼ 3ì”ì—ëŠ” 1700 x 3 = 5100ì›ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.35\n",
      "response = [4500ì› (ë§›ìˆì„ê±°ì—ìš”!)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 1.11\n",
      "response = [Question:\n",
      "- what is the most appropriate choice to answer this question? Explain your choice.\n",
      "- if you choose 3 1.5 then why? What about the first sentence? Why you think the price should be 5.5?\n",
      "\n",
      "Answer:\n",
      "- 3x1.5 = 3.75 + 5.5\n",
      "\n",
      "I think that the most appropriate answer is to use 3x1.5 = 3.75 + 5.5\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfff6ac03e964f96be4ec09fb2af39f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 7.12\n",
      "response = [\n",
      "Sure, I can help you with that.\n",
      "\n",
      "--\n",
      "\n",
      "For the case of 'ì•„ì´ìŠ¤ ë¼í…Œ 3ì”ì— ì–¼ë§ˆ', you can make a calculation on your own or you can just get the answer from me.\n",
      "\n",
      "--\n",
      "\n",
      "If you want to make the calculation on your own, here is how it goes.\n",
      "\n",
      "First, we need to know how much the price is for 1ì”, 2ì”, and 3ì”.\n",
      "\n",
      "1ì”: 1000ì›\n",
      "2ì”: 1200ì›\n",
      "3ì”: 1400ì›\n",
      "\n",
      "Next, we need to multiply the price of 1ì” by the number of 3ì” we want to buy.\n",
      "\n",
      "1000ì› x 3 = 3000ì›\n",
      "\n",
      "That's it! You can calculate the price of 3ì” on your own.\n",
      "\n",
      "--\n",
      "\n",
      "But, if you don't want to do the calculation, I can give you the exact price of 3ì”.\n",
      "\n",
      "For 3ì”, the price is]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2ì°¨ ì‹œë„\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28d94d23-2a63-4afb-943d-6ce6808b332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.47\n",
      "response = [ì¹´í˜ë¼ë–¼ 1 ì”ì— 1200ì›ì´ë¼ê³  í•˜ë‹ˆ, ì•„ì´ìŠ¤ ì¹´í˜ë¼ë–¼ 1 ì”ì— 1200 + 500 = 1700ì›ì´ë¼ê³ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì—ëŠ” 1700 * 3 = 5100ì›ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.93\n",
      "response = [4800ì›  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.48\n",
      "response = [ì¹´í˜ë¼ë–¼ 1ì”ì— 1200ì› + ì•„ì´ìŠ¤ ì¶”ê°€í•˜ë©´ 500ì› = 1700ì›\n",
      "ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì—ëŠ” 3 x 1700ì› = 5100ì›!]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.76\n",
      "response = [3ì”ì˜ ì•„ì´ìŠ¤ ì¹´í˜ë¼ë…¼ ê°€ê²©ì€ ì´ 3600ì›ì…ë‹ˆë‹¤. (ê° 1ì”ì— 1200ì› * 3 + ì¶”ê°€ì ì¸ ì•„ì´ìŠ¤ 500ì› * 3)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 0.42\n",
      "response = [Question: The correct answer is: 3\n",
      "\n",
      "## The solution is  1200 + 1400 + 1300 + 1000 = 5000\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90313698281a43d6b724d2ad76d04674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.19\n",
      "response = [\n",
      "You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3ì°¨ ì‹œë„\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f6944-ee1a-4f54-a7cb-1fdb127ce995",
   "metadata": {},
   "source": [
    "\n",
    "# 3.5) ì˜ì–´ë¡œ ë°”ê¾¸ì–´ì„œ ì§ˆì˜í•´ ë³´ê¸°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e492d5e-b304-4a4f-9eae-a61f2e2a9d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.42\n",
      "response = [ì•„ì´ìŠ¤ ì¹´í˜æ‹‰í…Œ 3ì”ì€ ê°ê° $1.2 + $0.5 = $1.7, $1.2 + $0.5 = $1.7, $1.2 + $0.5 = $1.7ë¡œ ê°€ê²©ì´ ë©ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 0.13\n",
      "response = [6 dollars  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 2.01\n",
      "response = [Caffelatte + Iced costs 1.2 dollar per cup, so 3 cups will be 3.6 dollars.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 0.46\n",
      "response = [3ì” ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œì˜ ê°€ê²©ì€ 1ì”ë‹¹ 1.2ë‹¬ëŸ¬ + 0.5ë‹¬ëŸ¬(ì•„ì´ìŠ¤) * 3 = 4.8ë‹¬ëŸ¬ì…ë‹ˆë‹¤.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.23\n",
      "response = [Question: What is the price of the 3rd cup of espresso? (1.4*3) is equal to 4.2 dollar. So the answer should be 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: 3ì”ì— 0.5ì´ ë”í•´ì ¸ì„œ ì–¼ë§ˆë‚˜ ë ê¹Œ?\n",
      "Answer directly: 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œì— ì»¤í”¼ìˆŸê°€ë½ í•œ ì”ì„ ë”í•´ì£¼ì„¸ìš”?\n",
      "Answer directly: 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: ì»¤í”¼ìˆŸê°€ë½ í•œ ì”ì„ ë¹¼ì£¼ì„¸ìš”?\n",
      "Answer directly: 3.8\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: ì»¤í”¼ìˆŸê°€ë½ í•œ ì”ì— ì–¼ë§ˆë‚˜ ë“¤ì–´ê°ˆê¹Œ? ì»¤í”¼ìˆŸê°€ë½ì„ 0.5ë¶„ê¸°í•œ 3ì”ì— ì–¼ë§ˆë‚˜ ë“¤ì–´ê°ˆê¹Œ?\n",
      "Answer directly: 3.8\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8946202165e043ed983af2eafd01e75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 1.68\n",
      "response = [\n",
      "Dear user,\n",
      "I am sorry for misunderstanding you. I misunderstood that you were asking me to help you in calculating the price of your cup of coffee. Please find the exact amount of money you have paid.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the total amount of money you paid for your coffee today?]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.5) ë°˜ë³µì ì¸ í…ŒìŠ¤íŠ¸í•˜ë„ë¡ ìˆ˜ì • (template + contextë¥¼ ë³´ì¶©í•´ì„œ ì§ˆì˜í•˜ëŠ” ë°©ë²•)\n",
    "\n",
    "#ì‹œë„ 1\n",
    "# ì •ë‹µ ëª¨ë¸: ì—†ë‹¤\n",
    "#query = \"ì•„ì´ìŠ¤ ì¹´í˜ë¼í…Œ 3ì”ì— ì–¼ë§ˆí•˜ë‚˜ìš”?\"\n",
    "\n",
    "#ì‹œë„ 2\n",
    "# ì •ë‹µ ëª¨ë¸: ì—†ë‹¤\n",
    "#query = \"how much do I buy 3 cups of iced-caffelatte?\"\n",
    "\n",
    "#enhanced_context_text = None\n",
    "enhanced_context_text = ['the price of a cup of americano is 1.0 dollar', \n",
    "                         'the price of a cup of caffelatte is 1.2 dollar', \n",
    "                         'the price of a cup of espresso is 1.4 dollar', \n",
    "                         'the price of a cup of cafuchino is 1.3 dollar', \n",
    "                         'the iced is added 0.5 dollar per a cup', \n",
    "                         ]\n",
    "\n",
    "# ì •ë‹µì€ 3 x (1.2 + 0.5) = 5.1ì›\n",
    "\n",
    "# 1ì°¨ ì‹œë„\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f97b3-a815-4749-8a9a-337a35d783a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymychabot",
   "language": "python",
   "name": "mychabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
