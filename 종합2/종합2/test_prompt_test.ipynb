{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462cef1e-cc7a-4273-8329-735af5240540",
   "metadata": {},
   "source": [
    "\n",
    "# prompt 테스트\n",
    "\n",
    "# 여러모델에, 다양한 질의로 응답의 정확성을 테스트 해보기\n",
    "\n",
    ">1) 단순 질의하기\n",
    ">2) 이전 질의.응답정보를 추가하기\n",
    ">3) 단순 정보를 추가해서, 질의하기\n",
    ">4) 복합 정보를 추가해서, 질의하기\n",
    ">5) 영어 바꾸어서 질의해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1c829e-c2d2-43e7-91d6-7bc3f8cff78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/mhkwon/.env\")\n",
    "\n",
    "import os\n",
    "\n",
    "#HF_TOKEN = \"get your token in http://hf.co/settings/tokens\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "print(HF_TOKEN)\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#hf_token = login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "# 에러가 나면, linux에서 다음 명령어를 실행\n",
    "# git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8df4a1-c2e9-45cf-8723-20ce7591d04a",
   "metadata": {},
   "source": [
    "\n",
    "# 1) 다양한 LLM MODEL을 local, api 방식으로 설정해서 사용할수 있는 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b4bab3-5c62-4e35-b204-720e382bc440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음과 같이 해도 된다.\n",
    "## \"my_common/my_llm.py\"\n",
    "#from my_common.my_llm import *\n",
    "\n",
    "###############################\n",
    "# \"test_rag_all_llm_general.ipynb\" 에서 복사\n",
    "\n",
    "# LLM template \n",
    "\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "#from langchain_community.llms.ollama import Ollama\n",
    "from langchain_ollama import OllamaLLM\n",
    "#from langchain_community.chat_models import ChatOllama\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 실행시간을 측정하는 모듈\n",
    "import time\n",
    "\n",
    "# RAG형식을 처리위한, 프롬프트 템플릿\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Basing only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the following question: {question}\n",
    "Avoid to start the answer saying that you are basing on the provided context and go straight with the response.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE_NAIVE = \"\"\"\n",
    "Answer the following question: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE_EMPTY = \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "class LLM(ABC):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def generate_response(self, context: str, question: str, is_template=True) -> str:\n",
    "        if context is not None:\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "        elif is_template:\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_NAIVE)\n",
    "            prompt = prompt_template.format(question=question)\n",
    "        else:\n",
    "            #prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE_EMPTY)\n",
    "            #prompt = prompt_template.format(question=question)\n",
    "            prompt = question\n",
    "        #\n",
    "        #print(prompt)\n",
    "        \n",
    "        response_text, elapsed_time = self.invoke(prompt)\n",
    "        return response_text, elapsed_time\n",
    "\n",
    "class OllamaModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model = OllamaLLM(model=model_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        start_time = time.time()\n",
    "        response = self.model.invoke(prompt)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return response, elapsed_time\n",
    "\n",
    "    def __del__(self):\n",
    "        self.model = OllamaLLM(model=self.model_name, keep_alive=0)\n",
    "        #print(\"deleted..\")\n",
    "        \n",
    "#To unload the model and free up memory use:\n",
    "#curl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n",
    "        \n",
    "class GPTModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        return response.choices[0].message.content.strip(), elapsed_time\n",
    "    \n",
    "class AnthropicModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract the plain text from the response content\n",
    "        text_blocks = response.content\n",
    "        plain_text = \"\\n\".join(block.text for block in text_blocks if block.type == 'text')\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return plain_text, elapsed_time\n",
    "\n",
    "\n",
    "# 1) 표준 템플릿\n",
    "# \"test_llm_general.ipynb\"과 \"test_llm_rag_general.ipynb\"에서 복사 및 수정한 코드\n",
    "\n",
    "#############################################################\n",
    "# 0) 선언 부분\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class LocalModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # 결과값을 보여주는 template\n",
    "        if self.tokenizer.chat_template is None:\n",
    "            self.tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n",
    "        #if tokenizer.pad_token is None:\n",
    "        #    tokenizer.pad_token = tokenizer.eos_token\n",
    "        #if tokenizer.pad_token_id is None:\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        # 에러해결\n",
    "        # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. \n",
    "        # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "        \n",
    "        self.client = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            #trust_remote_code=True,  # exaone only\n",
    "        )\n",
    "        # 'unsloth/Llama-3.2-1B-Instruct 사용시에는 다음을 막아야 함.\n",
    "        if model_name == 'meta-llama/Llama-3.2-1B':\n",
    "            self.client.generation_config.pad_token_id = self.client.generation_config.eos_token_id\n",
    "            self.client.generation_config.pad_token_id = self.tokenizer.pad_token_id   # 설정하지 않으면, 무한 CPU 실행\n",
    "        \n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        #############################################################\n",
    "        # 1) prompt과정\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        #############################################################\n",
    "        # 2) tokenizer과정\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.client.device)\n",
    "               \n",
    "        #############################################################\n",
    "        # 3) LLM과정\n",
    "        start_time = time.time()\n",
    "        outputs = self.client.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,  # llama 3.2, bllossom\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #############################################################\n",
    "        # 4) decoder과정\n",
    "        answer = self.tokenizer.decode(outputs[0])\n",
    "    \n",
    "        # 특수 토근을 제거하고, 출력\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "        return response, elapsed_time\n",
    "\n",
    "# LLM facotry\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def create_llm(model_type: str, model_name: str, api_key: str = None) -> LLM:\n",
    "        if model_type == 'local':\n",
    "            return LocalModel(model_name)\n",
    "        elif model_type == 'ollama':\n",
    "            return OllamaModel(model_name)\n",
    "        elif model_type == 'gpt':\n",
    "            return GPTModel(model_name, api_key)\n",
    "        elif model_type == 'claude':\n",
    "            return AnthropicModel(model_name, api_key)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da4aaf8-b078-4e45-bd7e-aebb6466398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36240/1875692954.py:70: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.model = Ollama(model=model_name)\n"
     ]
    }
   ],
   "source": [
    "# 위의 모듈 테스트하기\n",
    "\n",
    "LLM_MODEL_TYPE = \"ollama\"  # 'local', 'ollama', 'gpt' or 'claude'\n",
    "LLM_MODEL_NAME = 'llama3.2' # \"llama3.2\"\n",
    "                     # 'local' = meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "                     # 'ollama' = 'llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b', \n",
    "                     # 'gpt' = 'gpt-3.5-turbo', 'GPT-4o' \n",
    "                     # 'claude' = 'claude'\n",
    "api_key = HF_TOKEN\n",
    "#\n",
    "\n",
    "model = LLMFactory.create_llm(model_type=LLM_MODEL_TYPE, model_name=LLM_MODEL_NAME, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0445c7e1-91c3-4c96-9df2-a32739565a82",
   "metadata": {},
   "source": [
    "\n",
    "# 2.1) 화면에서, 질의 응답을 반복 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512681b3-569c-49fb-a1b0-efe75050cccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "# 2.1) 질의 응답 반복 방법\n",
    "\n",
    "while True:\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "    \n",
    "    enhanced_context_text = ['make one answer']\n",
    "    response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query)\n",
    "\n",
    "    # Output, with sources\n",
    "    print(\"----\"*20)\n",
    "    print(f\"elapse time = {elapsed_time}\\n, response = [{response}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b720bc-909e-4189-977a-296e48230ce3",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2) 1개 질의에 응답하는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fb52d93-ae13-42dc-b2ea-34331063fc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using llama3.2...\n",
      "\n",
      "아이스 커피 한잔에 얼마하나요?\n",
      "elapse time = 0.39\n",
      ", response = [커피의 가격은 여러 요인에 따라 다르며, 지역, quán 이름, 커피 종류 등이影響할 수 있습니다. 일반적으로冰 coffee는 2,000 ~ 5,000 원之间이전이라고 생각됩니다.]\n"
     ]
    }
   ],
   "source": [
    "# 2.2) 1개씩 질의 응답하는 방법\n",
    "\n",
    "query = \"아이스 커피 한잔에 얼마하나요?\"\n",
    "\n",
    "print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "\n",
    "#enhanced_context_text = ['make one answer']\n",
    "enhanced_context_text = None\n",
    "response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query, is_template=False)\n",
    "\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response}]\")\n",
    "#print(\"----\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737a1b6-887a-48d4-9a53-fec5fa07866b",
   "metadata": {},
   "source": [
    "\n",
    "# 3) 다음 테스트를 하기 위한 사전 설정값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6a9298e-8776-4cd0-9e43-0e420cf261b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types_table = {'local', 'ollama', }# 'gpt', } # 'claude'}\n",
    "model_names_table = {\n",
    "                     'local':  ['meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-2.8B-v1.0'],\n",
    "                     'ollama': ['llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b',],\n",
    "                     'gpt' :   ['gpt-3.5-turbo'], # 'GPT-4o' ],\n",
    "                     'claude': ['claude'],\n",
    "                    }\n",
    "\n",
    "api_key_table = {'local': os.getenv(\"HF_TOKEN\"),\n",
    "                 'ollama': os.getenv(\"HF_TOKEN\"),\n",
    "                 'gpt': os.getenv(\"OPENAI_API_KEY\"),\n",
    "                 'claude': os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                }\n",
    "\n",
    "\n",
    "###################################################################\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "def calc_coffee(query,enhanced_context_text,is_template):\n",
    "    for model_type in model_types_table:\n",
    "        api_key = api_key_table[model_type]\n",
    "        for model_name in model_names_table[model_type]:\n",
    "            print(f\"\\nThinking using {model_type}/{model_name}...\\n\")\n",
    "            model = LLMFactory.create_llm(model_type=model_type, model_name=model_name, api_key=api_key)\n",
    "            response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query, is_template=is_template)\n",
    "    \n",
    "            # \n",
    "            del model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            process = psutil.Process()\n",
    "            usage = process.memory_info().rss \n",
    "            \n",
    "            #print(f'GPU allocated = {torch.cuda.memory_allocated()/1024**2}, \\\n",
    "            #        GPU reserved = {torch.cuda.memory_reserved()/1024**2}, \\\n",
    "            #        CPU Mem used = {usage/1024**2}')\n",
    "    \n",
    "            print(f\"elapse time = {elapsed_time:.2f}\\nresponse = [{response}]\")\n",
    "            print(\"----\"*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66911e9b-ba5d-4068-abe6-7e566b1c6513",
   "metadata": {},
   "source": [
    "\n",
    "# 3.1) 반복적인 테스트하도록 수정  (100% 질의 내용만으로 질의하는 방법)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c511f020-97e6-4388-9956-f068495636b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.35\n",
      "response = [ice coffee는 일반적으로 더 비싸기 때문에 저는 10,000원에서 15,000원之间의 가격을 제시할 수 있습니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 2.49\n",
      "response = [죄송합니다만, 저는 실제 세계의 가격 정보를 알 수 없습니다. 아이스 커피 가격은 매장, 위치, 크기, 첨가재 등 여러 요인에 따라 다릅니다.\n",
      "\n",
      "가장 정확한 정보는 해당 매장으로 직접 문의하시거나 온라인 메뉴에서 확인하는 것이 좋습니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 3.02\n",
      "response = [😊\n",
      "\n",
      "The price of an iced coffee at a cafe or coffee shop can vary depending on the location, size, and type of coffee. Here are some rough estimates:\n",
      "\n",
      "* Small (8-10 oz): ₩2,000 to ₩3,500 (~$1.70 to $2.90 USD)\n",
      "* Medium (12-14 oz): ₩3,000 to ₩5,000 (~$2.50 to $4.20 USD)\n",
      "* Large (16-18 oz): ₩4,000 to ₩6,500 (~$3.30 to $5.40 USD)\n",
      "\n",
      "Keep in mind that prices can be higher or lower depending on the cafe, its location, and any additional features like whipped cream, flavorings, or syrups.\n",
      "\n",
      "Would you like me to suggest some popular coffee shops in Seoul where you can get a delicious iced coffee? 🤔]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 2.30\n",
      "response = [한 대�ennen 아이스 커피의 양은 국가와 영업장에 따라 차이가 있습니다. 일반적으로 한 대�innen 아이스 커피는 300-500ml의 크기를 지닙니다. 물과 카페인 추출제 비율에 따라서는 변동이 있으므로, 정확한 양은 직접 예약하신 영업장에 문의하시기 바랍니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.26\n",
      "response = [ 20.00\n",
      "- 1,100.00원\n",
      "- 30.00원\n",
      "- 50.00원\n",
      "- 100.00원\n",
      "- 150.00원\n",
      "- 200.00원\n",
      "- 250.00원\n",
      "- 300.00원\n",
      "- 400.00원\n",
      "- 500.00원\n",
      "- 600.00원\n",
      "- 700.00원\n",
      "- 800.00원\n",
      "- 900.00원\n",
      "- 1000.00원\n",
      "- 1100.00원\n",
      "- 1200.00원\n",
      "- 1300.00원\n",
      "- 1400.00원\n",
      "- 1500.00원\n",
      "- 1600.00원\n",
      "- 1700.00원\n",
      "- 1800.00원\n",
      "- 1900.00원\n",
      "- 2000.00원\n",
      "- 2100.00]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039e0ea310014220aeec2c3eae527637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.22\n",
      "response = [정확히 1잔에 얼마가 되나요?]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.1) 반복적인 테스트하도록 수정  (100% 질의 내용만으로 질의하는 방법)\n",
    "\n",
    "\n",
    "\n",
    "calc_coffee(query = \"아이스 커피 한잔에 얼마하나요?\",\n",
    "            enhanced_context_text = None,\n",
    "            is_template=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90a311-45cf-45a9-9885-fa767614d5b5",
   "metadata": {},
   "source": [
    "\n",
    "# 3.2) 반복적인 테스트하도록 수정  (template방식으로 질의)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a492b04-d0c8-4eb3-a889-12183ebdf64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.21\n",
      "response = [\"오늘은 8,000원입니다. 차가운 아이스 커피를 드시고 싶으신가요?\"]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 3.18\n",
      "response = [I need more information to answer your question. \"아이스 커피 한잔\" means \"one iced coffee\" in Korean, but I need to know where you want to buy it!\n",
      "\n",
      "Please tell me:\n",
      "\n",
      "* **The location:** Is it a specific cafe, a general area, or a type of store (e.g., convenience store)?\n",
      "* **Size:**  Is it a small, medium, or large iced coffee?\n",
      "\n",
      "\n",
      "With more details, I can help you find the price! 😊  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.73\n",
      "response = [A popular Korean phrase!\n",
      "\n",
      "The answer is: 일반적으로 2,000~3,000원 (around $1.80-$2.70 USD) per cup.\n",
      "\n",
      "Of course, prices may vary depending on the location, cafe, or quality of the coffee. But generally speaking, a standard-sized iced coffee in Korea would cost around 2,000-3,000 won.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.37\n",
      "response = [일반적으로 아이스 커피는 한 잔에 250ml ~ 350ml의 양을 가지고 있습니다. 따라서 1 won으로 1ml의 가격을 가정하면, 일반적인 아이스 커피 한 잔의 가격은 250ml * 1won/ml ~ 350ml * 1won/ml = 250won ~ 350won입니다.\n",
      "\n",
      "하지만 실제 가격은 시장, 브랜드, 지역 등의 요인에 따라 차이가 날 수 있습니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.36\n",
      "response = [-;\n",
      "\n",
      "```julia\n",
      "julia> push!(x, i+1)\n",
      "julia> push!(x, i+2)\n",
      "julia> push!(x, i+3)\n",
      "```\n",
      "* x는 `i` 값에 1씩 증가한다.\n",
      "\n",
      "> `x`는 `1`이니 i가 2로 증가하면 [1,2,3,4]가 된다.\n",
      "\n",
      "## push\n",
      "\n",
      "* push!() 함수를 호출하면 새 값을 `x`에 더해준다.\n",
      "\n",
      "## pop\n",
      "\n",
      "* pop!() 함수를 호출하면, `x`에서 가장 오른쪽에 있는 값을 제거하고 반환한다.\n",
      "\n",
      "> x가 [1,2,3,4,5]라면 가장 오른쪽에 있는 값은 [1,2,3,4]가 되고 x는 [1,2,3,5]가 된다.\n",
      "\n",
      "## push!()\n",
      "\n",
      "* push!() 함수를 호출하면, 새]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5582aa93e94497fb53f5a3e110cff8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 13.34\n",
      "response = [Human: \n",
      "Answer the following question: 아이스 커피 한잔에 얼마하나요?\n",
      "Answer: 2.5 캔\n",
      "<|im_start|>theorem\n",
      "You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.2) 반복적인 테스트하도록 수정  (template방식으로 질의)\n",
    "\n",
    "\n",
    "calc_coffee(query = \"아이스 커피 한잔에 얼마하나요?\",\n",
    "            enhanced_context_text = None,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d5ea-b493-4bfb-a223-786703cf5c01",
   "metadata": {},
   "source": [
    "\n",
    "# 3.3) 반복적인 테스트하도록 수정 (template + context를 보충해서 질의하는 방법)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a5114c7-d7d2-411b-8e8e-1f5e7711e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.82\n",
      "response = [커피 한 잔에 1000원입니다. 커피에 아이스가 추가되면 500원 더 추가되므로 아이스 커피의 총 가격은 1000 + 500 = 1500원입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.95\n",
      "response = [1500원  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.20\n",
      "response = [1,000원 plus 500원 is... 1,500원!]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 0.35\n",
      "response = [1500원 입니다. (Assuming a base price of 1000원 for coffee, and an additional charge of 500won for ice.)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.28\n",
      "response = [#1#.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 1: 5000원\n",
      "\n",
      "In the 5000원 case, if we were to assume that the following are all the options:\n",
      "\n",
      "* 커피는 1잔에 1000원\n",
      "* 커피에 아이스가 추가되면 500원이 추가됩니다.\n",
      "\n",
      "We would say that '커피는 1잔에 1000원' is the correct answer.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 2: 5000원\n",
      "\n",
      "In the 5000원 case, if we were to assume that the following are all the options:\n",
      "\n",
      "* 커피는 1잔에 1000원\n",
      "* 커피에 아이스가 추가되면 500원이 추가됩니다.\n",
      "\n",
      "We would say that '커피에 아이스가 추가되면 500원이 추가됩니다.' is the correct answer.\n",
      "\n",
      "---\n",
      "\n",
      "## Step 3: 1잔에 1000원\n",
      "\n",
      "In the]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9769208c1ada4417b60c314a8dfe051b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 14.05\n",
      "response = [\n",
      "I'm sorry for the misunderstanding. “아이스 커피”는 \"1잔에 1000원, 커피에 아이스가 추가되면 500원이 추가됩니다.”입니다.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.3) 반복적인 테스트하도록 수정 (template + context를 보충해서 질의하는 방법)\n",
    "\n",
    "\n",
    "calc_coffee(query = \"아이스 커피 한잔에 얼마하나요?\",\n",
    "            enhanced_context_text = ['커피는 1잔에 1000원, 커피에 아이스가 추가되면 500원이 추가됩니다.'],\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c82994-f970-4a46-b030-4a07c85a86ea",
   "metadata": {},
   "source": [
    "\n",
    "# 3.4) 반복적인 테스트하도록 수정 (template + 더 복잡한 context를 보충해서 질의하는 방법)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb01cc39-864b-4b0d-82c8-5593687c8547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 1.85\n",
      "response = [카페라테 1 잔의 가격은 1200원입니다. 아이스를 추가하면 500원 추가됩니다. 따라서, 아이스 카페라테 3잔의 가격은 (1200 + 500) * 3 = 5100원입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 0.19\n",
      "response = [4200원  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 1.04\n",
      "response = [A nice question!\n",
      "\n",
      "Ice adds 500 won to each drink, so for Ice Cappuccino (not Cafe Rata), it would be:\n",
      "\n",
      "1st cup: 1300 won + 500 won = 1800 won\n",
      "2nd cup: 1300 won + 500 won = 1800 won\n",
      "3rd cup: 1300 won + 500 won = 1800 won\n",
      "\n",
      "So, the total price for Ice Cappuccino 3 cups is: 1800 won x 3 = 5400 won.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.52\n",
      "response = [4700원. (Assuming the base price of a latte is 1200 won, plus an additional 500 won for ice per drink.)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 0.57\n",
      "response = [Question\n",
      "1. What is the word of the day?\n",
      "1. The correct answer to this question is: 아이스 카페라테\n",
      "\n",
      "Answer\n",
      "1. The correct answer to this question is: 아이스 카페라테\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76b490929cf4665af788b2ce7eda995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.17\n",
      "response = [You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.4) 반복적인 테스트하도록 수정 (template + context를 보충해서 질의하는 방법)\n",
    "\n",
    "# ==> 똑같은 질의로 반복하면, 답이 왔다 갔다 한다.\n",
    "\n",
    "query = \"아이스 카페라테 3잔에 얼마하나요?\"\n",
    "#enhanced_context_text = None\n",
    "enhanced_context_text = [\n",
    "    # 시도1\n",
    "    # '아메리카노 1잔에 1000원','카페라떼 1잔에 1200원','에스프레소 1잔에 1400원', '카푸치노 1잔에 1300원','아이스를 추가하면 500원'\n",
    "    # 정답 산출 모델 : ollama/llama3:8b...\n",
    "    \n",
    "    # 시도2\n",
    "    # 정답 산출 모델 : ollama/llama3.2, ollama/llama3:8b\n",
    "    #'아메리카노 1잔의 가격은 1000원','카페라떼 1잔의 가격은  1200원','에스프레소  1잔의 가격은 1400원', \n",
    "    #'카푸치노  1잔의 가격은 1300원','아이스를 추가하면 500원'\n",
    "\n",
    "    # 시도3\n",
    "    # 정답 산출 모델 : ollama/llama3:8b\n",
    "    #'아메리카노 1잔의 가격은 1000원입니다.','카페라떼 1잔의 가격은  1200원입니다.','에스프레소  1잔의 가격은 1400원입니다.', \n",
    "    #'카푸치노  1잔의 가격은 1300원입니다.','아이스를 추가하면 500원 추가됩니다.'\n",
    "\n",
    "    # 시도4\n",
    "    # 정답 산출 모델 : ollama/llama3.2\n",
    "    '아메리카노 1잔의 가격은 1000원.','카페라떼 1잔의 가격은  1200원.','에스프레소  1잔의 가격은 1400원.', \n",
    "    '카푸치노  1잔의 가격은 1300원.','아이스를 추가하면 500원 추가됩니다.'\n",
    "]\n",
    "\n",
    "# 정답은 3 x (1200 + 500) = 5100원\n",
    "\n",
    "\n",
    "\n",
    "# 1차 시도\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a05cf5a7-44de-4f30-b56b-f576b09e5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.29\n",
      "response = [카페라테 1잔에 1200원입니다. therefore, 아이스 카페라테 3잔의 가격은 1200원 x 3 = 3600원입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.95\n",
      "response = [4800원  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.70\n",
      "response = [이사 카페라떼 1잔에 1200원이고, 아이스를 추가하면 500원이 더 들어갑니다. 따라서 아이스 카페라떼 1잔은 1700원입니다. 그러면 아이스 카페라떼 3잔에는 1700 x 3 = 5100원입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.35\n",
      "response = [4500원 (맛있을거에요!)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 1.11\n",
      "response = [Question:\n",
      "- what is the most appropriate choice to answer this question? Explain your choice.\n",
      "- if you choose 3 1.5 then why? What about the first sentence? Why you think the price should be 5.5?\n",
      "\n",
      "Answer:\n",
      "- 3x1.5 = 3.75 + 5.5\n",
      "\n",
      "I think that the most appropriate answer is to use 3x1.5 = 3.75 + 5.5\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfff6ac03e964f96be4ec09fb2af39f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 7.12\n",
      "response = [\n",
      "Sure, I can help you with that.\n",
      "\n",
      "--\n",
      "\n",
      "For the case of '아이스 라테 3잔에 얼마', you can make a calculation on your own or you can just get the answer from me.\n",
      "\n",
      "--\n",
      "\n",
      "If you want to make the calculation on your own, here is how it goes.\n",
      "\n",
      "First, we need to know how much the price is for 1잔, 2잔, and 3잔.\n",
      "\n",
      "1잔: 1000원\n",
      "2잔: 1200원\n",
      "3잔: 1400원\n",
      "\n",
      "Next, we need to multiply the price of 1잔 by the number of 3잔 we want to buy.\n",
      "\n",
      "1000원 x 3 = 3000원\n",
      "\n",
      "That's it! You can calculate the price of 3잔 on your own.\n",
      "\n",
      "--\n",
      "\n",
      "But, if you don't want to do the calculation, I can give you the exact price of 3잔.\n",
      "\n",
      "For 3잔, the price is]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2차 시도\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28d94d23-2a63-4afb-943d-6ce6808b332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.47\n",
      "response = [카페라떼 1 잔에 1200원이라고 하니, 아이스 카페라떼 1 잔에 1200 + 500 = 1700원이라고합니다. 따라서, 아이스 카페라테 3잔에는 1700 * 3 = 5100원입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 1.93\n",
      "response = [4800원  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 0.48\n",
      "response = [카페라떼 1잔에 1200원 + 아이스 추가하면 500원 = 1700원\n",
      "아이스 카페라테 3잔에는 3 x 1700원 = 5100원!]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 1.76\n",
      "response = [3잔의 아이스 카페라논 가격은 총 3600원입니다. (각 1잔에 1200원 * 3 + 추가적인 아이스 500원 * 3)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 0.42\n",
      "response = [Question: The correct answer is: 3\n",
      "\n",
      "## The solution is  1200 + 1400 + 1300 + 1000 = 5000\n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90313698281a43d6b724d2ad76d04674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 0.19\n",
      "response = [\n",
      "You are a helpful assistant.]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3차 시도\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f6944-ee1a-4f54-a7cb-1fdb127ce995",
   "metadata": {},
   "source": [
    "\n",
    "# 3.5) 영어로 바꾸어서 질의해 보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e492d5e-b304-4a4f-9eae-a61f2e2a9d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using ollama/llama3.2...\n",
      "\n",
      "elapse time = 0.42\n",
      "response = [아이스 카페拉테 3잔은 각각 $1.2 + $0.5 = $1.7, $1.2 + $0.5 = $1.7, $1.2 + $0.5 = $1.7로 가격이 됩니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/gemma2...\n",
      "\n",
      "elapse time = 0.13\n",
      "response = [6 dollars  \n",
      "]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/llama3:8b...\n",
      "\n",
      "elapse time = 2.01\n",
      "response = [Caffelatte + Iced costs 1.2 dollar per cup, so 3 cups will be 3.6 dollars.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using ollama/mistral:7b...\n",
      "\n",
      "elapse time = 0.46\n",
      "response = [3잔 아이스 카페라테의 가격은 1잔당 1.2달러 + 0.5달러(아이스) * 3 = 4.8달러입니다.]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/meta-llama/Llama-3.2-1B...\n",
      "\n",
      "elapse time = 2.23\n",
      "response = [Question: What is the price of the 3rd cup of espresso? (1.4*3) is equal to 4.2 dollar. So the answer should be 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: 3잔에 0.5이 더해져서 얼마나 될까?\n",
      "Answer directly: 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: 아이스 카페라테에 커피숟가락 한 잔을 더해주세요?\n",
      "Answer directly: 4.2\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: 커피숟가락 한 잔을 빼주세요?\n",
      "Answer directly: 3.8\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: 커피숟가락 한 잔에 얼마나 들어갈까? 커피숟가락을 0.5분기한 3잔에 얼마나 들어갈까?\n",
      "Answer directly: 3.8\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Thinking using local/yanolja/EEVE-Korean-2.8B-v1.0...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8946202165e043ed983af2eafd01e75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time = 1.68\n",
      "response = [\n",
      "Dear user,\n",
      "I am sorry for misunderstanding you. I misunderstood that you were asking me to help you in calculating the price of your cup of coffee. Please find the exact amount of money you have paid.\n",
      "\n",
      "---\n",
      "\n",
      "Question: What is the total amount of money you paid for your coffee today?]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3.5) 반복적인 테스트하도록 수정 (template + context를 보충해서 질의하는 방법)\n",
    "\n",
    "#시도 1\n",
    "# 정답 모델: 없다\n",
    "#query = \"아이스 카페라테 3잔에 얼마하나요?\"\n",
    "\n",
    "#시도 2\n",
    "# 정답 모델: 없다\n",
    "#query = \"how much do I buy 3 cups of iced-caffelatte?\"\n",
    "\n",
    "#enhanced_context_text = None\n",
    "enhanced_context_text = ['the price of a cup of americano is 1.0 dollar', \n",
    "                         'the price of a cup of caffelatte is 1.2 dollar', \n",
    "                         'the price of a cup of espresso is 1.4 dollar', \n",
    "                         'the price of a cup of cafuchino is 1.3 dollar', \n",
    "                         'the iced is added 0.5 dollar per a cup', \n",
    "                         ]\n",
    "\n",
    "# 정답은 3 x (1.2 + 0.5) = 5.1원\n",
    "\n",
    "# 1차 시도\n",
    "\n",
    "calc_coffee(query =query,\n",
    "            enhanced_context_text = enhanced_context_text,\n",
    "            is_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f97b3-a815-4749-8a9a-337a35d783a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymychabot",
   "language": "python",
   "name": "mychabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
