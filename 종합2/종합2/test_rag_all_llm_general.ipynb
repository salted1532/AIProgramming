{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06faccf3-ef5d-44fc-b4a8-f722af214337",
   "metadata": {},
   "source": [
    "\n",
    "# LLM - RAG - GENERAL - TEMPLATE\n",
    "- 그동안 배운 RAG, Vectorstore, Loader등의 함수를 모두 포함한 코드\n",
    "- 여기서는 pdf파일을 \"data\"폴더에 넣어두고 사용한다.\n",
    "- 아래를 단계별로 실행하면, pdf을 읽어 loader해서, split해서 vectorstore에 저장하고,\n",
    "- 질의를 하면, 1) 프롬프트를 추가하고, 2) vdb에서 유사질의를 가져와서, 3) LLM에 질의를 하고, 4) 그 결과를 응답으로 처리하도록 되어 있다.\n",
    "\n",
    "# \"rag_all_llm_general\"\n",
    "\n",
    "\n",
    "# 폴더구성\n",
    "- \"data\" : pdf 또는 다른 문서\n",
    "- \"test_vdb_\"+LLM_MODEL_TYPE  : vetorstore가 저장되는 폴더\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074c9e5d-92c4-46f5-a3d7-0ddb1ccec972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN  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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/mhkwon/.env\")   # 각자 이전에 만들어둔 \".env\"파일이 있는 곳으로 변경 사용한다.\n",
    "\n",
    "import os\n",
    "\n",
    "# 다음 3개의 API KEY는 해당 사이트에 가서 등록하면, 무료 사용이 가능하다.\n",
    "# 각자 linux에 홈디렉토리에, 지난번에 배운대로 \".env\"파일에 추가해둔다.\n",
    "\n",
    "HF_TOKEN=os.getenv('HF_TOKEN')\n",
    "LANGCHAIN_API_KEY=os.getenv('LANGCHAIN_API_KEY')\n",
    "ANTHROPIC_API_KEY=os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "print('HF_TOKEN ', HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d93d902-7193-406f-a035-a3bf3617c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.8\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "print(langchain.__version__)\n",
    "\n",
    "# v0.3.8에서 동작\n",
    "# check date = 2024.11.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c985f2-79f8-4967-9692-539b34d3a3ef",
   "metadata": {},
   "source": [
    "\n",
    "# 1) LLM template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be1f009-ee7e-4169-bca7-1575cd7b0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM template \n",
    "\n",
    "\n",
    "import anthropic\n",
    "from openai import OpenAI\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "#from langchain_community.chat_models import ChatOllama\n",
    "#from langchain_anthropic import ChatAnthropic\n",
    "#from langchain_openai import ChatOpenAI\n",
    "#\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 실행시간을 측정하는 모듈\n",
    "import time\n",
    "\n",
    "# RAG형식을 처리위한, 프롬프트 템플릿\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Basing only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the following question: {question}\n",
    "Avoid to start the answer saying that you are basing on the provided context and go straight with the response.\n",
    "\"\"\"\n",
    "\n",
    "class LLM(ABC):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    @abstractmethod\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        pass\n",
    "\n",
    "    def generate_response(self, context: str, question: str) -> str:\n",
    "        if context is not None:\n",
    "            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "            prompt = prompt_template.format(context=context, question=question)\n",
    "        #\n",
    "        print(prompt)\n",
    "        \n",
    "        response_text, elapsed_time = self.invoke(prompt)\n",
    "        return response_text, elapsed_time\n",
    "\n",
    "class OllamaModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model = Ollama(model=model_name)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        start_time = time.time()\n",
    "        response = self.model.invoke(prompt)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return response, elapsed_time\n",
    "\n",
    "class GPTModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=150,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        return response.choices[0].message.content.strip(), elapsed_time\n",
    "    \n",
    "class AnthropicModel(LLM):\n",
    "    def __init__(self, model_name: str, api_key: str):\n",
    "        super().__init__(model_name)\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model_name,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Extract the plain text from the response content\n",
    "        text_blocks = response.content\n",
    "        plain_text = \"\\n\".join(block.text for block in text_blocks if block.type == 'text')\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        return plain_text, elapsed_time\n",
    "\n",
    "\n",
    "# 1) 표준 템플릿\n",
    "# \"test_llm_general.ipynb\"과 \"test_llm_rag_general.ipynb\"에서 복사 및 수정한 코드\n",
    "\n",
    "#############################################################\n",
    "# 0) 선언 부분\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class LocalModel(LLM):\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__(model_name)\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # 결과값을 보여주는 template\n",
    "        if self.tokenizer.chat_template is None:\n",
    "            self.tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n",
    "        #if tokenizer.pad_token is None:\n",
    "        #    tokenizer.pad_token = tokenizer.eos_token\n",
    "        #if tokenizer.pad_token_id is None:\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        # 에러해결\n",
    "        # The attention mask is not set and cannot be inferred from input because pad token is same as eos token. \n",
    "        # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
    "        \n",
    "        self.client = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            #trust_remote_code=True,  # exaone only\n",
    "        )\n",
    "        # 'unsloth/Llama-3.2-1B-Instruct 사용시에는 다음을 막아야 함.\n",
    "        if model_name == 'meta-llama/Llama-3.2-1B':\n",
    "            self.client.generation_config.pad_token_id = self.client.generation_config.eos_token_id\n",
    "            self.client.generation_config.pad_token_id = self.tokenizer.pad_token_id   # 설정하지 않으면, 무한 CPU 실행\n",
    "        \n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        #############################################################\n",
    "        # 1) prompt과정\n",
    "        messages = [\n",
    "            #{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        #############################################################\n",
    "        # 2) tokenizer과정\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.client.device)\n",
    "               \n",
    "        #############################################################\n",
    "        # 3) LLM과정\n",
    "        start_time = time.time()\n",
    "        outputs = self.client.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "            pad_token_id = self.tokenizer.eos_token_id,  # llama 3.2, bllossom\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        #############################################################\n",
    "        # 4) decoder과정\n",
    "        answer = self.tokenizer.decode(outputs[0])\n",
    "    \n",
    "        # 특수 토근을 제거하고, 출력\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "        return response, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ac6457-11ee-44ef-927f-786c904670fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM facotry\n",
    "\n",
    "class LLMFactory:\n",
    "    @staticmethod\n",
    "    def create_llm(model_type: str, model_name: str, api_key: str = None) -> LLM:\n",
    "        if model_type == 'local':\n",
    "            return LocalModel(model_name)\n",
    "        elif model_type == 'ollama':\n",
    "            return OllamaModel(model_name)\n",
    "        elif model_type == 'gpt':\n",
    "            return GPTModel(model_name, api_key)\n",
    "        elif model_type == 'claude':\n",
    "            return AnthropicModel(model_name, api_key)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea97e12-e1c3-487d-b7d6-35600452e0c1",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1) 다음의 파라미터로 llm을 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38aa8247-343e-489e-a262-6851548ed60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17359/4196162413.py:46: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  self.model = Ollama(model=model_name)\n"
     ]
    }
   ],
   "source": [
    "LLM_MODEL_TYPE = \"ollama\"  # 'local', 'ollama', 'gpt' or 'claude'\n",
    "LLM_MODEL_NAME = 'llama3.2' # \"llama3.2\"\n",
    "                     # 'local' = meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "                     # 'ollama' = 'llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b', \n",
    "                     # 'gpt' = 'gpt-3.5-turbo', 'GPT-4o' \n",
    "                     # 'claude' = 'claude'\n",
    "api_key = HF_TOKEN\n",
    "#\n",
    "\n",
    "model = LLMFactory.create_llm(model_type=LLM_MODEL_TYPE, model_name=LLM_MODEL_NAME, api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b37fbea-d27f-41c6-b236-88de28ae0b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OllamaModel at 0x7fa3a1dab070>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd260e6-d0fe-457e-a864-e7795a7e15ed",
   "metadata": {},
   "source": [
    "\n",
    "# 1.2) 단순 query로 만들어진 llm를 테스트해본다.\n",
    "\n",
    "- 한글은 이상하게 나오지만,\n",
    "- 영어는 잘된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c9ea75-ba97-4300-839d-db204b100f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  the capital of korea?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using llama3.2...\n",
      "\n",
      "Human: \n",
      "Basing only on the following context:\n",
      "\n",
      "['make one answer']\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: the capital of korea?\n",
      "Avoid to start the answer saying that you are basing on the provided context and go straight with the response.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "elapse time = 1.1938908100128174\n",
      ", response = [Seoul.]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "    \n",
    "    enhanced_context_text = ['make one answer']\n",
    "    response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query)\n",
    "\n",
    "    # Output, with sources\n",
    "    print(\"----\"*20)\n",
    "    print(f\"elapse time = {elapsed_time}\\n, response = [{response}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d722a9c-2fa3-444b-ae64-135c983140cb",
   "metadata": {},
   "source": [
    "\n",
    "# 2) vectorstore관련 함수 만들기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68b3f05-9a03-4e28-995f-6500a5153c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema import Document\n",
    "#from embeddings.embeddings import Embeddings  # OLD\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\"\"\"\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    "    # With the `text-embedding-3` class\n",
    "    # of models, you can specify the size\n",
    "    # of the embeddings you want returned.\n",
    "    # dimensions=1024\n",
    ")\n",
    "\"\"\"\n",
    "class RAGRetriever:\n",
    "    def __init__(self, vector_db_path: str, embedding_model_name: str, api_key: str):\n",
    "        self.vector_db_path = vector_db_path\n",
    "        if embedding_model_name == 'openai':\n",
    "            embeddings = OpenAIEmbeddings()\n",
    "        else:\n",
    "            model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        \n",
    "        #self.embedding_function = embeddings.get_embedding_function()\n",
    "        self.embedding_function = embeddings\n",
    "        self.db = Chroma(persist_directory=self.vector_db_path, embedding_function=self.embedding_function)\n",
    "\n",
    "    def query(self, query_text: str, k: int = 4):\n",
    "        # compute similarity between embeddings of query and of pdf text chunks\n",
    "        results = self.db.similarity_search_with_score(query_text, k=k)\n",
    "        return results\n",
    "\n",
    "    def format_results(self, results: list[tuple[Document, float]]):\n",
    "        enhanced_context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        sources = set(self.format_source(doc.metadata) for doc, _score in results)  # set to ensure uniqueness\n",
    "        return enhanced_context_text, list(sources)\n",
    "\n",
    "    def format_source(self, metadata: dict):\n",
    "        source = metadata.get(\"source\", \"unknown\")\n",
    "        page = metadata.get(\"page\", \"unknown\")\n",
    "        filename = source.split(\"\\\\\")[-1]  # extract filename\n",
    "        return f\"{filename} page {page}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a982e147-ba27-4c77-ae8a-5b04fa932c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17359/2352367719.py:38: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.db = Chroma(persist_directory=self.vector_db_path, embedding_function=self.embedding_function)\n"
     ]
    }
   ],
   "source": [
    "vector_db_path = 'test_vdb_'+LLM_MODEL_TYPE\n",
    "\n",
    "\n",
    "retriever = RAGRetriever(vector_db_path=vector_db_path, embedding_model_name=LLM_MODEL_TYPE, api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f87f5-833f-47d9-bd3d-87f8c6f49575",
   "metadata": {},
   "source": [
    "# 2.1) universal loader\n",
    "- 지난번에 만든것 사용\n",
    "- \"test_loader_universal.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a6f844d-fbbf-47bb-aee3-9416702a6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "\n",
    "ALLOW_TEXT = [\".txt\", \".md\"]\n",
    "ALLOW_HTML = [\".html\", \".htm\"]\n",
    "ALLOW_PDF = [\".pdf\"]\n",
    "ALLOW_CSV = [\".csv\"]\n",
    "ALLOW_JSON = [\".json\"]\n",
    "ALLOW_ZIP = [\".zip\"]\n",
    "\n",
    "\n",
    "def universal_file_loader(source):\n",
    "    filename, file_extension = os.path.splitext(source)\n",
    "    file_extension = file_extension.lower()\n",
    "    #print(f\"filename= {filename}, extension= {file_extension}\")\n",
    "    \n",
    "    data, loader = '', None\n",
    "    if file_extension in ALLOW_TEXT :\n",
    "        loader = TextLoader(source)\n",
    "    elif file_extension in ALLOW_HTML:\n",
    "        loader = BSHTMLLoader(source)\n",
    "    elif file_extension in ALLOW_PDF:\n",
    "        loader = PyPDFLoader(source)\n",
    "    elif file_extension in ALLOW_CSV:\n",
    "        loader = CSVLoader(file_path=source)\n",
    "    elif file_extension in ALLOW_JSON:\n",
    "        #loader = JSONLoader(file_path=source,\n",
    "        #                    jq_schema='.content',\n",
    "        #                    #text_content=False,\n",
    "        #                    json_lines=True)\n",
    "        data = json.loads(Path(source).read_text())\n",
    "    else:\n",
    "        print(\"Not recognize file type :\", file_extension)\n",
    "    if loader:\n",
    "        data = loader.load()\n",
    "    return data\n",
    "\n",
    "# \"http\", \"https\", \"ftp\"\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "def universal_url_loader(source):\n",
    "    loader = RecursiveUrlLoader(\n",
    "        source,\n",
    "        prevent_outside=True,\n",
    "        #base_url=\"https://docs.python.org\",\n",
    "        link_regex=r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*(?=index)[^\"]*)\"',\n",
    "        #exclude_dirs=['https://docs.python.org/3.9/faq']\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import requests\n",
    "\n",
    "def universal_zip_loader(file_url):\n",
    "    url = requests.get(file_url)\n",
    "    zipfile = ZipFile(BytesIO(url.content))\n",
    "    zipfile.extractall(\"/tmp\")\n",
    "    docs = []\n",
    "    for file_name in zipfile.namelist():\n",
    "        doc = universal_file_loader(\"/tmp/\"+file_name)\n",
    "        if doc:\n",
    "            #docs.append([doc])\n",
    "            docs.extend(doc)\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "def universal_dir_loader(source):\n",
    "    #print(\"*start=\",source)\n",
    "    #c_dir = os.getcwd() \n",
    "    dir_list = os.listdir(source) # 현재 폴더에 있는것만 읽어 들임.\n",
    "    docs = []\n",
    "    dir_list.sort()\n",
    "    for file_name in dir_list:\n",
    "        doc = None\n",
    "        if file_name[0] == \".\":\n",
    "            continue\n",
    "        #print(\"name=\",file_name)\n",
    "        if os.path.isdir(file_name):\n",
    "            doc = universal_dir_loader(source+\"/\"+file_name)\n",
    "        else:\n",
    "            doc = universal_file_loader(source+\"/\"+file_name)\n",
    "        if doc:\n",
    "            #docs.append(doc)\n",
    "            docs.extend(doc)\n",
    "    return docs\n",
    "\n",
    "\n",
    "regex = (\"((http|https)://)(www.)?\" + \"[a-zA-Z0-9@:%._\\\\+~#?&//=]\" +\n",
    "             \"{2,256}\\\\.[a-z]\" + \"{2,6}\\\\b([-a-zA-Z0-9@:%\" + \"._\\\\+~#?&//=]*)\")\n",
    "p = re.compile(regex)\n",
    "\n",
    "# 자동으로 체크해서, type을 결정하도록 수정\n",
    "def universal_loader_v2(source):\n",
    "    docs = []\n",
    "    if(re.search(p, source)):\n",
    "        docs = universal_url_loader(source)\n",
    "    elif os.path.isdir(source):\n",
    "        docs = universal_dir_loader(source)\n",
    "    elif os.path.isfile(source):\n",
    "        docs = universal_file_loader(source)        \n",
    "    else:\n",
    "        filename, file_extension = os.path.splitext(source)\n",
    "        file_extension = file_extension.lower()\n",
    "        if file_extension in ALLOW_ZIP:\n",
    "            docs = universal_zip_loader(source)\n",
    "    return docs\n",
    "\n",
    "# type을 수동으로 지정해주면 사용\n",
    "def universal_loader_v1(source, type='file'):\n",
    "    docs = []\n",
    "    if type == 'file':\n",
    "        docs = universal_file_loader(source)\n",
    "    elif type == 'dir':\n",
    "        docs = universal_dir_loader(source)\n",
    "    elif type == 'url':\n",
    "        docs = universal_url_loader(source)\n",
    "    elif type == 'zip':\n",
    "        docs = universal_zip_loader(source)\n",
    "    return docs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6135d4e-3130-41c7-8ee3-5a6c945288ff",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2)  vdb store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "747c65b4-081f-4b20-be47-45f712bd8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def reset_databases(reset_choice):\n",
    "    if reset_choice in [\"openai\", \"both\"]:\n",
    "        if ask_to_clear_database(\"openai\"):\n",
    "            print(\"✨ Rebuilding OpenAI Database\")\n",
    "            clear_database(\"openai\")\n",
    "            #rebuild_database(\"openai\")\n",
    "\n",
    "    if reset_choice in [\"ollama\", \"both\"]:\n",
    "        if ask_to_clear_database(\"ollama\"):\n",
    "            print(\"✨ Rebuilding Ollama Database\")\n",
    "            clear_database(\"ollama\")\n",
    "            #rebuild_database(\"ollama\")\n",
    "\n",
    "def ask_to_clear_database(embedding_model):\n",
    "    response = input(f\"Do you want to override the existing {embedding_model} database? (yes/no): \").strip().lower()\n",
    "    return response == 'yes'\n",
    "\n",
    "def load_documents(path):\n",
    "    #document_loader = PyPDFDirectoryLoader(path)\n",
    "    #docs = document_loader.load()\n",
    "    docs = universal_loader_v2(path)\n",
    "    return docs\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def add_to_chroma(chunks: list[Document], db):\n",
    "    # calculate Page IDs\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # only add documents that don't exist in the DB\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"➕ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "    # create IDs like \"data/alpha_society.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # if the page ID is the same as the last one, increment the index\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # calculate the unique chunk ID\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # add it to the page meta-data\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def clear_database(embedding_model):\n",
    "    if embedding_model == \"openai\":\n",
    "        db_path = 'test_vdb_'+embedding_model\n",
    "    elif embedding_model == \"ollama\":\n",
    "        db_path = 'test_vdb_'+embedding_model\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding model specified.\")\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        shutil.rmtree(db_path)\n",
    "\n",
    "def rebuild_database(retriever, vectordbpath, document_path):\n",
    "\n",
    "    # load the existing database\n",
    "    db = Chroma(persist_directory=vectordbpath, embedding_function=retriever.embedding_function)\n",
    "\n",
    "    # create (or update) the data store\n",
    "    documents = load_documents(document_path)\n",
    "    chunks = split_documents(documents)\n",
    "    add_to_chroma(chunks, db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3279917-fd7b-47ae-9094-558934eab8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../rag-conversational-agent/data'  # pdf file이 들어 있는 폴더\n",
    "\n",
    "def start_vdb(retriever, vectordbpath, document_path):\n",
    "    rebuild_database(retriever, vectordbpath, document_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe579db4-4414-4905-b8df-8a1f91ae08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_vdb():\n",
    "    reset_databases(LLM_MODEL_TYPE)  #'ollama', 'openai' 'both'\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "affa0578-f90f-453d-b9d3-a220ab32f546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 9\n",
      "✅ No new documents to add\n"
     ]
    }
   ],
   "source": [
    "# vdb에 파일 추가하기\n",
    "\n",
    "#start_vdb(retriever=retriever, vectordbpath=vector_db_path, document_path=path)\n",
    "\n",
    "# 바로 위 함수를 호출하지 않고, 다음처럼 debug하기 위해서, 풀어서 사용\n",
    "db = Chroma(persist_directory=vector_db_path, embedding_function=retriever.embedding_function)\n",
    "documents = load_documents(path)\n",
    "chunks = split_documents(documents)\n",
    "add_to_chroma(chunks, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba98c586-eafb-40a5-8a2d-b415305663fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1fb5c20a-7aa4-4ab5-be01-776e372d4061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../rag-conversational-agent/data/alpha_society.pdf', 'page': 0}, page_content=\"Alpha Corporation  \\nAlpha Corporation, founded in 2010, is a trailblazer in the field of renewable energy. Headquartered \\nin San Francisco, California, Alpha has quickly risen to prominence with its innovative approaches \\nto sustainable power solutions. The company's core busin ess revolves around the development and \\ndeployment of solar and wind energy technologies, aiming to reduce global dependence on fossil \\nfuels.  \\nMembers:  \\n• CEO - Lisa Thompson : With a Ph.D. in Renewable Energy Engineering from Stanford \\nUniversity, Lisa brings a wealth of knowledge and experience to Alpha. Her leadership has \\nbeen instrumental in steering the company towards groundbreaking advancements in \\nrenewable technology.  \\n• CTO - Dr. Michael Anderson : A former NASA scientist, Michael specializes in advanced \\nmaterials and nanotechnology. His contributions to Alpha’s research and development \\nefforts have led to the creation of highly efficient solar panels and wind turbines.  \\n• COO - Maria Gonzalez : Maria, with her extensive background in supply chain \\nmanagement and logistics, ensures that Alpha’s operations run smoothly. She oversees the \\nproduction and distribution of Alpha’s products, ensuring they reach markets worldwide \\nefficiently.  \\n• Head of Research - Dr. Richard Lee : Richard, a renowned physicist, leads Alpha's \\nresearch team. His work focuses on improving energy conversion efficiency and developing \\nnew storage solutions to make renewable energy more accessible and reliable.  \\nAlpha Corporation’s mission is to make renewable energy affordable and accessible to everyone. \\nThey have pioneered several initiatives, including community solar programs and partnerships with \\ngovernments and NGOs to bring clean energy to underserved areas . Alpha’s innovative spirit and \\ncommitment to sustainability continue to drive their growth and impact in the energy sector.  \\n \"),\n",
       " Document(metadata={'source': '../rag-conversational-agent/data/beta_society.pdf', 'page': 0}, page_content=\"Beta Enterprises  \\nBeta Enterprises, established in 2005, is a leading player in the field of biotechnology and \\npharmaceuticals. Based in Cambridge, Massachusetts, Beta has earned a reputation for its cutting -\\nedge research and development in medical treatments and therapies.  Their core business involves \\nthe discovery and commercialization of novel drugs to treat rare and chronic diseases.  \\nMembers:  \\n• CEO - Dr. Emily Parker : Emily, with her dual degrees in Medicine and Business \\nAdministration from Harvard, has a unique blend of clinical and managerial expertise. \\nUnder her leadership, Beta has achieved several FDA approvals for its innovative drugs.  \\n• CSO - Dr. David Kim : David, a molecular biologist with over 20 years of experience, \\nheads Beta's scientific research. His team is responsible for several groundbreaking \\ndiscoveries in gene therapy and immunotherapy.  \\n• CFO - Sarah Johnson : Sarah, with her extensive background in finance and \\npharmaceuticals, manages Beta’s financial strategy. Her expertise ensures the company’s \\nrobust financial health and supports its research funding.  \\n• VP of Clinical Trials - Dr. Rachel Singh : Rachel, a seasoned clinical researcher, oversees \\nBeta’s clinical trial operations. Her team conducts rigorous testing to ensure the safety and \\nefficacy of Beta’s drug candidates.  \\nBeta Enterprises is dedicated to improving patient outcomes through innovative treatments. They \\ncollaborate with academic institutions, hospitals, and research organizations globally to advance \\ntheir research. Beta’s patient -centric approach and commitment  to excellence have made them a \\ntrusted name in the biotech industry.  \\n \"),\n",
       " Document(metadata={'source': '../rag-conversational-agent/data/gamma_society.pdf', 'page': 0}, page_content='Gamma Innovations  \\nGamma Innovations, founded in 2015, is a dynamic technology company specializing in artificial \\nintelligence and machine learning solutions. Located in the heart of Silicon Valley, Gamma is at the \\nforefront of the AI revolution, providing state -of-the-art software and services to businesses across \\nvarious sectors.  \\nMembers:  \\n• CEO - Jonathan Miller : Jonathan, a visionary entrepreneur with a background in computer \\nscience, founded Gamma to harness the potential of AI. His strategic vision has propelled \\nGamma to become a leader in AI innovation.  \\n• CIO - Dr. Samantha Lee : Samantha, an expert in machine learning and data analytics, \\nleads Gamma’s technological development. Her team focuses on creating scalable AI \\nsolutions that can be integrated into diverse business operations.  \\n• COO - Kevin Patel : Kevin, with his strong operational and management skills, ensures that \\nGamma’s projects are delivered on time and meet clients’ expectations. His efforts have \\nstreamlined Gamma’s processes, enhancing productivity and client satisfaction.  \\n• Head of AI Research - Dr. Arjun Desai : Arjun, a distinguished AI researcher, directs \\nGamma’s research initiatives. His work in neural networks and natural language processing \\nhas led to several patented technologies that form the backbone of Gamma’s offerings.  \\nGamma Innovations offers a suite of AI -powered tools and services, including predictive analytics, \\nautomated customer support, and intelligent data processing. They work with clients in finance, \\nhealthcare, retail, and more, helping them leverage AI to dri ve growth and efficiency. Gamma’s \\ncommitment to innovation and excellence positions them as a pivotal player in the tech industry’s \\nfuture.  \\n ')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0db41d8-164f-4559-a2a0-9d266b4c6daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../rag-conversational-agent/data/alpha_society.pdf', 'page': 0}, page_content=\"Alpha Corporation  \\nAlpha Corporation, founded in 2010, is a trailblazer in the field of renewable energy. Headquartered \\nin San Francisco, California, Alpha has quickly risen to prominence with its innovative approaches \\nto sustainable power solutions. The company's core busin ess revolves around the development and \\ndeployment of solar and wind energy technologies, aiming to reduce global dependence on fossil \\nfuels.  \\nMembers:  \\n• CEO - Lisa Thompson : With a Ph.D. in Renewable Energy Engineering from Stanford \\nUniversity, Lisa brings a wealth of knowledge and experience to Alpha. Her leadership has \\nbeen instrumental in steering the company towards groundbreaking advancements in \\nrenewable technology.  \\n• CTO - Dr. Michael Anderson : A former NASA scientist, Michael specializes in advanced \\nmaterials and nanotechnology. His contributions to Alpha’s research and development \\nefforts have led to the creation of highly efficient solar panels and wind turbines.  \\n• COO - Maria Gonzalez : Maria, with her extensive background in supply chain \\nmanagement and logistics, ensures that Alpha’s operations run smoothly. She oversees the \\nproduction and distribution of Alpha’s products, ensuring they reach markets worldwide \\nefficiently.  \\n• Head of Research - Dr. Richard Lee : Richard, a renowned physicist, leads Alpha's \\nresearch team. His work focuses on improving energy conversion efficiency and developing \\nnew storage solutions to make renewable energy more accessible and reliable.  \\nAlpha Corporation’s mission is to make renewable energy affordable and accessible to everyone. \\nThey have pioneered several initiatives, including community solar programs and partnerships with \\ngovernments and NGOs to bring clean energy to underserved areas . Alpha’s innovative spirit and \\ncommitment to sustainability continue to drive their growth and impact in the energy sector.  \\n \")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c47053b2-adca-439c-a62f-b8df45b06cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '../rag-conversational-agent/data/alpha_society.pdf', 'page': 0}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fe79c1b-d8bb-4294-8858-135327c0415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alpha Corporation  \\nAlpha Corporation, founded in 2010, is a trailblazer in the field of renewable energy. Headquartered \\nin San Francisco, California, Alpha has quickly risen to prominence with its innovative approaches \\nto sustainable power solutions. The company's core busin ess revolves around the development and \\ndeployment of solar and wind energy technologies, aiming to reduce global dependence on fossil \\nfuels.  \\nMembers:  \\n• CEO - Lisa Thompson : With a Ph.D. in Renewable Energy Engineering from Stanford \\nUniversity, Lisa brings a wealth of knowledge and experience to Alpha. Her leadership has \\nbeen instrumental in steering the company towards groundbreaking advancements in \\nrenewable technology.  \\n• CTO - Dr. Michael Anderson : A former NASA scientist, Michael specializes in advanced \\nmaterials and nanotechnology. His contributions to Alpha’s research and development \\nefforts have led to the creation of highly efficient solar panels and wind turbines.  \\n• COO - Maria Gonzalez : Maria, with her extensive background in supply chain \\nmanagement and logistics, ensures that Alpha’s operations run smoothly. She oversees the \\nproduction and distribution of Alpha’s products, ensuring they reach markets worldwide \\nefficiently.  \\n• Head of Research - Dr. Richard Lee : Richard, a renowned physicist, leads Alpha's \\nresearch team. His work focuses on improving energy conversion efficiency and developing \\nnew storage solutions to make renewable energy more accessible and reliable.  \\nAlpha Corporation’s mission is to make renewable energy affordable and accessible to everyone. \\nThey have pioneered several initiatives, including community solar programs and partnerships with \\ngovernments and NGOs to bring clean energy to underserved areas . Alpha’s innovative spirit and \\ncommitment to sustainability continue to drive their growth and impact in the energy sector.  \\n \""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "778b81ef-6923-49d4-804a-ec86027af59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to override the existing ollama database? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✨ Rebuilding Ollama Database\n"
     ]
    }
   ],
   "source": [
    "# vdb를 reset하는 명령어\n",
    "\n",
    "reset_vdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2421de-a803-4500-9c0c-08fbe3756daf",
   "metadata": {},
   "source": [
    "\n",
    "# 3) RAG방식 query\n",
    "\n",
    "- 다음은 위에서 추가한 pdf 파일을 기초로해서 응답이 만들어지는 과정\n",
    "- 위에 pdf파일을 한글로된 파일을 추가하고, 그 파일에 있는 내용으로 테스트를 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77967c09-ab72-4842-8e76-08cebcfbbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  the capital of america?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thinking using llama3.2...\n",
      "\n",
      "Human: \n",
      "Basing only on the following context:\n",
      "\n",
      "new storage solutions to make renewable energy more accessible and reliable.  \n",
      "Alpha Corporation’s mission is to make renewable energy affordable and accessible to everyone. \n",
      "They have pioneered several initiatives, including community solar programs and partnerships with \n",
      "governments and NGOs to bring clean energy to underserved areas . Alpha’s innovative spirit and \n",
      "commitment to sustainability continue to drive their growth and impact in the energy sector.\n",
      "\n",
      "---\n",
      "\n",
      "Alpha Corporation  \n",
      "Alpha Corporation, founded in 2010, is a trailblazer in the field of renewable energy. Headquartered \n",
      "in San Francisco, California, Alpha has quickly risen to prominence with its innovative approaches \n",
      "to sustainable power solutions. The company's core busin ess revolves around the development and \n",
      "deployment of solar and wind energy technologies, aiming to reduce global dependence on fossil \n",
      "fuels.  \n",
      "Members:  \n",
      "• CEO - Lisa Thompson : With a Ph.D. in Renewable Energy Engineering from Stanford \n",
      "University, Lisa brings a wealth of knowledge and experience to Alpha. Her leadership has \n",
      "been instrumental in steering the company towards groundbreaking advancements in \n",
      "renewable technology.\n",
      "\n",
      "---\n",
      "\n",
      "Beta Enterprises  \n",
      "Beta Enterprises, established in 2005, is a leading player in the field of biotechnology and \n",
      "pharmaceuticals. Based in Cambridge, Massachusetts, Beta has earned a reputation for its cutting -\n",
      "edge research and development in medical treatments and therapies.  Their core business involves \n",
      "the discovery and commercialization of novel drugs to treat rare and chronic diseases.  \n",
      "Members:  \n",
      "• CEO - Dr. Emily Parker : Emily, with her dual degrees in Medicine and Business \n",
      "Administration from Harvard, has a unique blend of clinical and managerial expertise. \n",
      "Under her leadership, Beta has achieved several FDA approvals for its innovative drugs.  \n",
      "• CSO - Dr. David Kim : David, a molecular biologist with over 20 years of experience,\n",
      "\n",
      "---\n",
      "\n",
      "Answer the following question: the capital of america?\n",
      "Avoid to start the answer saying that you are basing on the provided context and go straight with the response.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "elapse time = 1.4390010833740234\n",
      ", response = [There is no information in the provided context about either Alpha Corporation or Beta Enterprises having a capital, let alone being the capital of America. The text only discusses their missions, initiatives, and key members.]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  quit\n"
     ]
    }
   ],
   "source": [
    "NUM_RELEVANT_DOCS = 3\n",
    "\n",
    "while True:\n",
    "    query = input(\"Query: \")\n",
    "    if len(query) == 0:\n",
    "        print(\"Please enter a question. Ctrl+C to Quit.\\n\")\n",
    "        continue\n",
    "    if query == 'quit':\n",
    "        break\n",
    "\n",
    "    print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "\n",
    "    #\n",
    "    results = retriever.query(query, k=NUM_RELEVANT_DOCS)\n",
    "    enhanced_context_text, sources = retriever.format_results(results)\n",
    "    #\n",
    "    response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query)\n",
    "\n",
    "    # Output, with sources\n",
    "    print(\"----\"*20)\n",
    "    print(f\"elapse time = {elapsed_time}\\n, response = [{response}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2da924-2b38-425e-b3fa-8de1e4a03baf",
   "metadata": {},
   "source": [
    "\n",
    "# 4) flask로 만들어 보기\n",
    "\n",
    "- 1) 위의 코드를 참고하고, 각자의 코드를 발전시키고,\n",
    "- 2) 수업 초기에 각자 flask기반으로 만들었던 코드에 합쳐서,\n",
    "- 3) 지난주 과제를 수행해보길 바란다.\n",
    "\n",
    "- flask는 jupyter에서 동작하지 않는다. 위의 모든 코드를 각 구분별로 파일을 만들어 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843ef3e-15e5-4e86-974b-2bb494e010e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymychabot",
   "language": "python",
   "name": "mychabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
