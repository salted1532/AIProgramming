{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ad7e52-4959-43fa-b3b3-147c57735c63",
   "metadata": {},
   "source": [
    "\n",
    "# chat_history 사용방법\n",
    "\n",
    "- prompt에서 history를 기억하고 대답할수 있도록 만들어주는 방법을 체크 해보자.\n",
    "- 많은 방법이 있다.\n",
    "\n",
    ">  1) OpenAI + langgraph로 해보기 (최신 langchain버전의 기능임)\n",
    ">  2) OpenAI + chat history\n",
    ">  3) Hugginface + chat_history로 해보기.\n",
    ">  - 한글은 잘안된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f588f7da-0b97-426b-a7a1-333bd22b5e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/mhkwon/.env\")\n",
    "\n",
    "import os\n",
    "\n",
    "#HF_TOKEN = \"get your token in http://hf.co/settings/tokens\"\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "print(HF_TOKEN)\n",
    "\n",
    "#from huggingface_hub import login\n",
    "#hf_token = login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "\n",
    "# 에러가 나면, linux에서 다음 명령어를 실행\n",
    "# git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc562007-8ce7-4fcd-91f4-44b9b1154a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.8'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f04af16-265d-480c-a3be-04eb5b944857",
   "metadata": {},
   "source": [
    "\n",
    "# 1) OpenAI + langgraph로 해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac94ce1-bb62-41f5-8ada-9211f80e638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/chatbots_memory.ipynb\n",
    "# 본 사이트에서는 trimming, summary등의 기능이 포함되어 있지만, 여기서는 제외.\n",
    "\n",
    "# 1) OpenAI로 해보기\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f70bbc-b814-47c8-82d0-4effc7d0df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said \"J'adore la programmation,\" which translates to \"I love programming\" in French.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a28033-0f36-494c-b583-7fd7d482a40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I said \"J\\'adore la programmation,\" which translates to \"I love programming\" in French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 56, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3de1288069', 'finish_reason': 'stop', 'logprobs': None}, id='run-31a24aee-62c8-4147-991b-8f0a56fdddf2-0', usage_metadata={'input_tokens': 56, 'output_tokens': 19, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6141609b-4f89-4bf7-aa0c-590ad4030f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic history management\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    print(messages)\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "# highlight-start\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "# highlight-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb75592a-be51-45c8-ac19-0c9fbe0873ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='3d2a9656-dbfe-4473-ac73-d28f3447f34b')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='3d2a9656-dbfe-4473-ac73-d28f3447f34b'),\n",
       "  AIMessage(content='In French, \"I love programming\" is translated as \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 35, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bcdbf964-edaf-4ef7-af9c-407896fdd17a-0', usage_metadata={'input_tokens': 35, 'output_tokens': 18, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7011899-53f4-4446-b268-6580c498813b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='3d2a9656-dbfe-4473-ac73-d28f3447f34b'), AIMessage(content='In French, \"I love programming\" is translated as \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 35, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bcdbf964-edaf-4ef7-af9c-407896fdd17a-0', usage_metadata={'input_tokens': 35, 'output_tokens': 18, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='fe0cf2c5-854e-4a42-9348-506713fef6ed')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='3d2a9656-dbfe-4473-ac73-d28f3447f34b'),\n",
       "  AIMessage(content='In French, \"I love programming\" is translated as \"J\\'adore programmer.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 35, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-bcdbf964-edaf-4ef7-af9c-407896fdd17a-0', usage_metadata={'input_tokens': 35, 'output_tokens': 18, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='fe0cf2c5-854e-4a42-9348-506713fef6ed'),\n",
       "  AIMessage(content='You just asked me to translate the phrase \"I love programming\" into French.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 68, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9269519e-4811-46b5-80c4-2e4a2b1db37b-0', usage_metadata={'input_tokens': 68, 'output_tokens': 16, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 질의가 무엇인지를 호출하는것\n",
    "\n",
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b786f328-b1b3-4a08-8b43-98d817b938a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant. Answer all questions to the best of your ability.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='2f32d119-85ea-4ac9-9e75-9e596c9f534e'), AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='896aec57-f625-4632-9848-f808b1178e24'), HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='5f72d9a1-4e45-486b-9c24-eac90c5fa310'), AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='6e183936-157b-4fbe-a37b-700571d0f928'), HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='a9ad38a8-a166-489d-9111-72e37d893e5e')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='2f32d119-85ea-4ac9-9e75-9e596c9f534e'),\n",
       "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='896aec57-f625-4632-9848-f808b1178e24'),\n",
       "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='5f72d9a1-4e45-486b-9c24-eac90c5fa310'),\n",
       "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='6e183936-157b-4fbe-a37b-700571d0f928'),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='a9ad38a8-a166-489d-9111-72e37d893e5e'),\n",
       "  AIMessage(content='Your name is Nemo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 66, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f3a8a423-1ba0-455d-af75-c896f2a7e1d1-0', usage_metadata={'input_tokens': 66, 'output_tokens': 6, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifying chat history\n",
    "\n",
    "demo_ephemeral_chat_history = [\n",
    "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
    "    AIMessage(content=\"Hello!\"),\n",
    "    HumanMessage(content=\"How are you today?\"),\n",
    "    AIMessage(content=\"Fine thanks!\"),\n",
    "]\n",
    "\n",
    "app.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history\n",
    "        + [HumanMessage(content=\"What's my name?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42549e8a-f15d-4129-8f75-ef6335c32b36",
   "metadata": {},
   "source": [
    "\n",
    "# 2) OpenAI + chat history\n",
    "- 원소스에는 아래 그림처럼 Retrieve까지 포함되어 있지만, 여기서는 QA로 수정했다.\n",
    "- \n",
    "[원소스보기](https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/)\n",
    "\n",
    "![LLM+RAG+WithHistory](https://python.langchain.com/v0.1/assets/images/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3fdd19-504c-487d-90f4-ab5dc04b6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = qa_prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3445d6eb-c3a2-408b-828d-680c9cc15b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Common ways of task decomposition include creating a work breakdown structure (WBS), identifying dependencies between tasks, breaking down tasks based on their priority or complexity, and assigning responsibilities for each sub-task to team members. Decomposing tasks can also involve using tools like mind maps, flowcharts, or task lists to visually represent the breakdown of the main task into smaller components. Additionally, iterative decomposition, where tasks are further broken down as the project progresses, can help in refining the understanding and execution of the overall task.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 125, 'total_tokens': 226, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-a82c055d-968b-44b6-81e4-fdfccf912d20-0' usage_metadata={'input_tokens': 125, 'output_tokens': 101, 'total_tokens': 226, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = question_answer_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "#chat_history.extend([HumanMessage(content=question), ai_msg_1.[\"answer\"]])\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = question_answer_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b72ca083-a1dc-4e44-b611-f5f89ee1e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Task decomposition is a problem-solving strategy that involves breaking down a complex task into smaller, more manageable sub-tasks. By dividing the task into smaller components, it becomes easier to understand and solve each part individually. This approach is commonly used in project management and computer programming.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 55, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-9c5a6b3b-8838-49b3-9350-f48a8eb8f99c-0', usage_metadata={'input_tokens': 55, 'output_tokens': 54, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "199a7503-2946-4a1e-93ba-af137ce4a87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a problem-solving strategy that involves breaking down a complex task into smaller, more manageable sub-tasks. By dividing the task into smaller components, it becomes easier to understand and solve each part individually. This approach is commonly used in project management and computer programming.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7f562-adad-444e-8ece-6564b93ce111",
   "metadata": {},
   "source": [
    "\n",
    "# 3) Huggingface + chat history\n",
    "- 위 소스에 huggingface를 결합해서 만듬.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f82fde7-3e80-4c8a-a7c0-f7390f89d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/linuxhdd/mychatbot/my_note\n",
      "\n",
      "Thinking using llama3.2...\n",
      "\n",
      "-------4-------- 아이스 커피 한잔에 얼마하나요?\n",
      "elapse time = 0.76\n",
      ", response = [ice coffee는 일반적인 커피와 달리, 冷める process가 더 적어서, 보통冰과 수을 섞어 만든 것입니다.\n",
      "\n",
      " 일반적으로 ice coffee의 가격은 커피의 가격에 비해 저렴합니다. 예를 들어, 한 잔의 일반적인 커피가 1,500원에서 3,000원까지 range이지만, ice coffee는 800원에서 2,000원까지 price range이 있습니다.\n",
      "\n",
      "위 giá_range은 general price range이며, actual price는 varied할 수 있습니다.]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 local my_llm을 제대로 읽어들이지는지 체크\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# \"my_common/my_llm.py\"\n",
    "from my_common.my_llm import *\n",
    "\n",
    "\n",
    "# 위의 모듈 테스트하기\n",
    "\n",
    "LLM_MODEL_TYPE = \"ollama\"  # 'local', 'ollama', 'gpt' or 'claude'\n",
    "LLM_MODEL_NAME = 'llama3.2' # \"llama3.2\"\n",
    "                     # 'local' = meta-llama/Llama-3.2-1B', 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "                     # 'ollama' = 'llama3.2', 'gemma2', 'llama3:8b', 'mistral:7b', \n",
    "                     # 'gpt' = 'gpt-3.5-turbo', 'GPT-4o' \n",
    "                     # 'claude' = 'claude'\n",
    "api_key = HF_TOKEN\n",
    "#\n",
    "\n",
    "model = LLMFactory.create_llm(model_type=LLM_MODEL_TYPE, model_name=LLM_MODEL_NAME, api_key=api_key)\n",
    "\n",
    "\n",
    "query = \"아이스 커피 한잔에 얼마하나요?\"\n",
    "\n",
    "print(f\"\\nThinking using {LLM_MODEL_NAME}...\\n\")\n",
    "\n",
    "#enhanced_context_text = ['make one answer']\n",
    "enhanced_context_text = None\n",
    "response, elapsed_time = model.generate_response(context=enhanced_context_text,  question=query, is_template=False)\n",
    "\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response}]\")\n",
    "#print(\"----\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63eaeb2b-ec3e-4024-8209-893f0238eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "condense_template = \"\"\"Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    {chat_history}\n",
    "    \n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(condense_template)\n",
    "\n",
    "#qa_template = qa_prompt.format(question=query, chat_history=chat_history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e29b7b2-61cd-4bb1-a1c2-15f22e8f7b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template=\"Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n    \\n    {chat_history}\\n    \\n    Human: {question}\\n    AI:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51c86654-b45c-4666-984c-1bb35a0fb9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------4-------- Human: Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    \n",
      "    []\n",
      "    \n",
      "    Human: What is Task Decomposition?\n",
      "    AI:\n",
      "elapse time = 0.42\n",
      ", response = [Task decomposition is a problem-solving technique where a complex task or project is broken down into smaller, manageable subtasks. This helps to organize the work, reduce complexity, and increase productivity. It's often used in project management, software development, and other fields where tasks need to be prioritized and executed efficiently.]\n"
     ]
    }
   ],
   "source": [
    "#from langchain_core.messages import HumanMessage\n",
    "\n",
    "question1 = \"What is Task Decomposition?\"\n",
    "\n",
    "qa_template1 = qa_prompt.format(question=question1, chat_history=chat_history)\n",
    "#response1, elapsed_time = model.generate_response(qa_template=qa_template1)\n",
    "response1, elapsed_time = model.generate_response(question=qa_template1, is_template=False)\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response1}]\")\n",
    "\n",
    "#chat_history.extend([HumanMessage(content=question1), response1])\n",
    "chat_history.append(f\"Human: {question1}, AI: {response1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f50e808b-04ff-40f5-8c39-2f1f0e23f551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Using the following conversation, answer friendly for the newest question. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\n    \\n    [HumanMessage(content=\\'What is Task Decomposition?\\', additional_kwargs={}, response_metadata={}), \"Task decomposition is a technique used in project management and problem-solving where a complex task or process is broken down into smaller, more manageable tasks. This helps to identify the specific steps required to complete the task, prioritize them, and assign resources accordingly. It\\'s often used to simplify complex projects and make them more achievable by dividing them into smaller, more digestible pieces!\"]\\n    \\n    Human: What is Task Decomposition?\\n    AI:'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_template1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8da75e05-2a46-43ac-b590-637df9c5d001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Human: What is Task Decomposition?, AI: Task decomposition is a problem-solving technique where a complex task or project is broken down into smaller, manageable subtasks. This helps to organize the work, reduce complexity, and increase productivity. It's often used in project management, software development, and other fields where tasks need to be prioritized and executed efficiently.\"]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06330eed-68ed-41ce-bf85-e7e9f5022821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a problem-solving technique where a complex task or project is broken down into smaller, manageable subtasks. This helps to organize the work, reduce complexity, and increase productivity. It's often used in project management, software development, and other fields where tasks need to be prioritized and executed efficiently.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adf3bfef-15d6-4067-8075-69a9e4e86ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------4-------- Human: Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    \n",
      "    []\n",
      "    \n",
      "    Human: What is Task Decomposition?\n",
      "    AI:\n",
      "elapse time = 0.47\n",
      ", response = [Task decomposition is a method used in project management and problem-solving where a large task or goal is broken down into smaller, more manageable sub-tasks. This technique helps to simplify complex tasks, make them easier to understand and complete, and ultimately lead to greater success and productivity. It involves identifying the individual steps required to achieve a specific objective and assigning responsibilities to each step.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "question2 = \"What are common ways of doing it?\"\n",
    "qa_template2 = qa_prompt.format(question=second_question, chat_history=chat_history)\n",
    "#response2, elapsed_time = model.generate_response(qa_template=qa_template2)\n",
    "response2, elapsed_time = model.generate_response(question=qa_template1, is_template=False)\n",
    "\n",
    "chat_history.append(f\"Human: {question2}, AI: {response2}\")\n",
    "\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response2}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "731a3557-f027-4bd3-88ce-909cfb8b7c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a method used in project management and problem-solving where a large task or goal is broken down into smaller, more manageable sub-tasks. This technique helps to simplify complex tasks, make them easier to understand and complete, and ultimately lead to greater success and productivity. It involves identifying the individual steps required to achieve a specific objective and assigning responsibilities to each step.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "240e380a-aeba-4e8f-8695-2fc4421bf33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Human: What is Task Decomposition?, AI: Task decomposition is a problem-solving technique where a complex task or project is broken down into smaller, manageable subtasks. This helps to organize the work, reduce complexity, and increase productivity. It's often used in project management, software development, and other fields where tasks need to be prioritized and executed efficiently.\",\n",
       " 'Human: What are common ways of doing it?, AI: Task decomposition is a method used in project management and problem-solving where a large task or goal is broken down into smaller, more manageable sub-tasks. This technique helps to simplify complex tasks, make them easier to understand and complete, and ultimately lead to greater success and productivity. It involves identifying the individual steps required to achieve a specific objective and assigning responsibilities to each step.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a8bd40b-c616-4beb-a82d-a07299579c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########한글 질문 ###########\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "condense_template = \"\"\"Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    {chat_history}\n",
    "    \n",
    "    Human: {question}\n",
    "    AI:\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(condense_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a72c7092-bf82-492c-9758-a69fc6d7daef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------4-------- Human: Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    \n",
      "    []\n",
      "    \n",
      "    Human: 나는 철수입니다. 지금 학교에 가고 있어요\n",
      "    AI:\n",
      "elapse time = 0.13\n",
      ", response = [안녕하세요 철수씨!學校가 어떻게 đi세요?]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question1 = \"나는 철수입니다. 지금 학교에 가고 있어요\"\n",
    "\n",
    "qa_template1 = qa_prompt.format(question=question1, chat_history=chat_history)\n",
    "#response1, elapsed_time = model.generate_response(qa_template=qa_template1)\n",
    "response1, elapsed_time = model.generate_response(question=qa_template1, is_template=False)\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response1}]\")\n",
    "\n",
    "#chat_history.extend([HumanMessage(content=question1), response1])\n",
    "chat_history.append(f\"Human: {question1}, AI: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "591ceb59-ae7d-4a63-aa8e-06cf828b8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------4-------- Human: Using the following conversation, answer friendly for the newest question. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "    \n",
      "    []\n",
      "    \n",
      "    Human: 나는 철수입니다. 지금 학교에 가고 있어요\n",
      "    AI:\n",
      "elapse time = 0.16\n",
      ", response = [hello! iron-san, how are you doing today? are you looking forward to school?]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "question2 = \"내 이름이 뭐죠?\"\n",
    "qa_template2 = qa_prompt.format(question=second_question, chat_history=chat_history)\n",
    "#response2, elapsed_time = model.generate_response(qa_template=qa_template2)\n",
    "response2, elapsed_time = model.generate_response(question=qa_template1, is_template=False)\n",
    "\n",
    "chat_history.append(f\"Human: {question2}, AI: {response2}\")\n",
    "\n",
    "print(f\"elapse time = {elapsed_time:.2f}\\n, response = [{response2}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de60e9c3-1da8-4901-9a71-2389b2398853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mymychabot",
   "language": "python",
   "name": "mychabot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
